{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02694201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "TRADENAME CLEANING & VALIDATION PIPELINE v5 (ENGLISH-ONLY, NO EMOJIS)\n",
    "===============================================================================\n",
    "\n",
    "Major changes from v4:\n",
    "\n",
    "  1) Output CSV contains ONLY confirmed rows (is_tradename = True)\n",
    "     - Not the whole dataset, not pending rows\n",
    "     - Each batch is APPENDED to the existing file (no rewrite)\n",
    "\n",
    "  2) Row order is preserved EXACTLY as in the original input CSV\n",
    "     - No alphabetical sorting\n",
    "     - start_from / end_at operate on this same preserved order\n",
    "\n",
    "  3) Order is guaranteed when splitting across workers\n",
    "     - Each worker processes a specific range from the SAME ordered-unique list\n",
    "     - The unique list order is identical across machines\n",
    "\n",
    "Output CSV:\n",
    "  - All original columns\n",
    "  - + new columns (tradename_cleaned, tradename_corrected, ...)\n",
    "  - Only rows where is_tradename = True and validated\n",
    "  - Each batch appends without rewriting the file\n",
    "\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# ==============================================================================\n",
    "# SETTINGS\n",
    "# ==============================================================================\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/DataDoseClean/Tradename Clean\"\n",
    "\n",
    "INPUT_CSV = os.path.join(BASE_DIR, \"DataDoseDataset_FinalV_Last.csv\")\n",
    "\n",
    "# This CSV is updated on every batch — always contains all confirmed rows\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, \"dataset_with_validated_tradenames.csv\")\n",
    "\n",
    "OUTPUT_JSON = os.path.join(BASE_DIR, \"tradenames_validated.json\")\n",
    "PROGRESS_FILE = os.path.join(BASE_DIR, \"tradename_pipeline_progress.json\")\n",
    "LOG_FILE = os.path.join(BASE_DIR, \"tradename_pipeline_log.txt\")\n",
    "\n",
    "# Keys are masked here for security (use your real keys locally)\n",
    "GROQ_API_KEYS = [\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "    \"gsk_***\",\n",
    "]\n",
    "\n",
    "_current_key_idx = 0\n",
    "_processed_count = 0\n",
    "ROTATE_EVERY = 10\n",
    "\n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
    "GROQ_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "\n",
    "# ==============================================================================\n",
    "# LOGGING\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"Print and append logs to a file (best-effort).\"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] [{level}] {msg}\"\n",
    "    print(line)\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1 — DEEP REGEX CLEANING\n",
    "# ==============================================================================\n",
    "#\n",
    "# Goal:\n",
    "#   - Keep:   Brand name + concentration\n",
    "#   - Remove: dosage form (tabs, caps, syrup, cream, F.C.tabs, ...)\n",
    "#   - Remove: pack counts / total volume (20 tabs, 120 ml, 100 gm, ...)\n",
    "#   - Remove: leading serial numbers (1 2 3 ...)\n",
    "#   - Remove: number words (one, two, three ...)\n",
    "#   - Remove: quality terms USP/BP/NA (u.s.p.xxiii, n/a yet, ...)\n",
    "#\n",
    "# Concentration examples to KEEP:\n",
    "#   500mg | 100mg/ml | 0.5% | 1000iu | 10meq | 250mcg | 50mg/5ml\n",
    "#\n",
    "# Pack/volume examples to REMOVE:\n",
    "#   120 ml | 100 gm | 30 tabs | 10 caps | 5 sachets\n",
    "#\n",
    "# Approach:\n",
    "#   - Temporarily replace concentrations with placeholders before removal steps,\n",
    "#     then restore them at the end.\n",
    "# ==============================================================================\n",
    "\n",
    "_DOSAGE_FORM_RE = re.compile(\n",
    "    r\"\"\"\n",
    "    \\b(?:\n",
    "      # Tablets\n",
    "      f\\.?c\\.?tab(?:let)?s?\\.?   |\n",
    "      film[- ]coated[- ]tab(?:let)?s?  |\n",
    "      tab(?:let)?s?              |\n",
    "\n",
    "      # Capsules\n",
    "      caps?(?:ule)?s?            |\n",
    "\n",
    "      # Suppositories\n",
    "      supp(?:ositorie)?s?        |\n",
    "      suppository                |\n",
    "\n",
    "      # Liquids / other forms\n",
    "      syrup | syr\\.?             |\n",
    "      suspension | susp\\.?       |\n",
    "      solution | sol\\.? | soln\\.? |\n",
    "      drops?                     |\n",
    "      injection | inj\\.?         |\n",
    "      infusion                   |\n",
    "\n",
    "      # Ampoules / vials\n",
    "      amp(?:oule)?s?             |\n",
    "      vial?s?                    |\n",
    "\n",
    "      # Sachets / effervescent\n",
    "      sachet?s?                  |\n",
    "      effervescent               |\n",
    "\n",
    "      # Topicals\n",
    "      cream | gel                |\n",
    "      oint(?:ment)?              |\n",
    "      lotion | spray             |\n",
    "      patch(?:es)?               |\n",
    "\n",
    "      # Other forms\n",
    "      powder | granule?s?        |\n",
    "      lozenge?s?                 |\n",
    "      inhal(?:er|ation)?         |\n",
    "      nebuli[sz]er               |\n",
    "\n",
    "      # Qualifiers (not part of the core name)\n",
    "      nasal | ophthalmic | otic  |\n",
    "      topical | oral             |\n",
    "      paed(?:iatric)?            |\n",
    "      pediatric | adult          |\n",
    "\n",
    "      # Quality abbreviations / notes\n",
    "      u\\.s\\.p\\.(?:xxiii)?        |\n",
    "      b\\.p\\.                     |\n",
    "      n/?a[- ]?yet               |\n",
    "\n",
    "      # Specific form phrase\n",
    "      effervescent[- ]tablet?s?\n",
    "    )\\b\n",
    "\"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    ")\n",
    "\n",
    "_CONCENTRATION_RE = re.compile(\n",
    "    r\"(?<!\\w)\"\n",
    "    r\"(\\d+(?:\\.\\d+)?)\"\n",
    "    r\"\\s{0,1}\"\n",
    "    r\"(\"\n",
    "    r\"mg(?:/\\d*\\.?\\d*\\s*m[lg])?\"\n",
    "    r\"|mcg|µg|ug\"\n",
    "    r\"|iu|meq|mmol\"\n",
    "    r\"|%\"\n",
    "    r\")\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "_COUNT_VOLUME_RE = re.compile(\n",
    "    r\"\"\"\n",
    "    \\s+\n",
    "    \\d+(?:\\.\\d+)?\n",
    "    \\s*\n",
    "    (?:\n",
    "      ml\\b | l\\b      |\n",
    "      gm?\\b | gms?\\b  |\n",
    "      kg\\b            |\n",
    "      pcs?\\b          |\n",
    "      units?\\b        |\n",
    "      x\\s*\\d+         |\n",
    "      \\*\\s*\\d+\n",
    "    )\n",
    "\"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    ")\n",
    "\n",
    "_NUMBER_WORDS_RE = re.compile(\n",
    "    r\"\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|hundred)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "_LEADING_NUMS_RE = re.compile(r\"^\\s*(?:\\d+[\\.\\-\\)]\\s*|\\d+\\s+)+\")\n",
    "\n",
    "_TRAILING_COUNT_RE = re.compile(r\"\\s+\\d+\\s*$\")\n",
    "\n",
    "\n",
    "def clean_tradename(raw) -> str:\n",
    "    \"\"\"\n",
    "    Clean a drug name while preserving concentration.\n",
    "\n",
    "    Output:\n",
    "      Brand Name + concentration only\n",
    "\n",
    "    Examples:\n",
    "      \"Abilify 15mg 30 F.c.tabs.\"              -> \"Abilify 15mg\"\n",
    "      \"Abramox 100mg/ml Syrup\"                 -> \"Abramox 100mg/ml\"\n",
    "      \"Abrammune 50 Mg 10 Caps.(n/a Yet)\"      -> \"Abrammune 50mg\"\n",
    "      \"Abimol Extra 20\"                        -> \"Abimol Extra\"\n",
    "      \"1 2 3 (one Two Three) Syrup 120 Ml\"     -> \"\"\n",
    "    \"\"\"\n",
    "    if raw is None or (isinstance(raw, float) and pd.isna(raw)):\n",
    "        return \"\"\n",
    "\n",
    "    t = str(raw).strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Remove HTML entities\n",
    "    t = re.sub(r\"&[a-zA-Z]+;\", \" \", t)\n",
    "\n",
    "    # 2) Remove unwanted symbols (keep: letters, digits, whitespace, - . / % mg/ml basics)\n",
    "    t = re.sub(r\"[^\\w\\s\\-\\./®™°%,()]\", \" \", t)\n",
    "\n",
    "    # 3) Temporarily store concentrations as placeholders\n",
    "    concentrations = []\n",
    "\n",
    "    def _save_conc(m):\n",
    "        start = m.start()\n",
    "        prefix = t[:start].rstrip()\n",
    "        if prefix and not re.search(r\"[a-zA-Z\\)]$\", prefix):\n",
    "            return m.group(0)\n",
    "\n",
    "        token = m.group(0).strip()\n",
    "        token = re.sub(r\"(\\d)\\s+(\\w)\", r\"\\1\\2\", token)\n",
    "        concentrations.append(token.lower())\n",
    "        return f\" __CONC_{len(concentrations) - 1}__ \"\n",
    "\n",
    "    t = _CONCENTRATION_RE.sub(_save_conc, t)\n",
    "\n",
    "    # 4) Remove total volume/pack count patterns\n",
    "    t = _COUNT_VOLUME_RE.sub(\" \", t)\n",
    "\n",
    "    # 5) Remove dosage form words\n",
    "    t = _DOSAGE_FORM_RE.sub(\" \", t)\n",
    "\n",
    "    # 6) Remove number words\n",
    "    t = _NUMBER_WORDS_RE.sub(\" \", t)\n",
    "\n",
    "    # 7) Remove leading serial numbers\n",
    "    t = _LEADING_NUMS_RE.sub(\"\", t)\n",
    "\n",
    "    # 8) Remove empty parentheses or parentheses containing only numbers/punctuation\n",
    "    t = re.sub(r\"\\(\\s*[\\d\\s\\.\\-]*\\s*\\)\", \" \", t)\n",
    "    t = re.sub(r\"\\(\\s*\\)\", \" \", t)\n",
    "\n",
    "    # 9) Normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # 10) Remove leftover standalone numbers (not concentration placeholders, not part of a name)\n",
    "    t = re.sub(\n",
    "        r\"\\s+(\\d+)(?!\\s*(?:mg|mcg|iu|meq|mmol|%|/|__CONC|\\w))\",\n",
    "        \"\",\n",
    "        t,\n",
    "    )\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # 10b) If the result contains no letters, return empty\n",
    "    if t and not re.search(r\"[a-zA-Z_]\", t):\n",
    "        return \"\"\n",
    "\n",
    "    # 11) Trim punctuation from ends\n",
    "    t = t.strip(\" .-/()\")\n",
    "\n",
    "    # 12) Restore saved concentrations\n",
    "    def _restore_conc(m):\n",
    "        idx = int(m.group(1))\n",
    "        return concentrations[idx] if idx < len(concentrations) else \"\"\n",
    "\n",
    "    t = re.sub(r\"__CONC_(\\d+)__\", _restore_conc, t)\n",
    "\n",
    "    # 13) Normalize whitespace again\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    t = t.strip(\" .-/()\")\n",
    "\n",
    "    # 14) Smart Title Case\n",
    "    words = []\n",
    "    for w in t.split():\n",
    "        if re.match(r\"^\\d\", w) or re.match(r\".*\\d.*(?:mg|ml|mcg|iu|meq|%)\", w, re.I):\n",
    "            words.append(w.lower())\n",
    "        elif w.isupper() and 1 < len(w) <= 5:\n",
    "            words.append(w)\n",
    "        elif re.match(r\"^[A-Za-z]\\d+$\", w):\n",
    "            words.append(w[0].upper() + w[1:])\n",
    "        else:\n",
    "            words.append(w.capitalize())\n",
    "\n",
    "    t = \" \".join(words).strip()\n",
    "    return t if len(t) >= 2 else \"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EGYPT MARKET CONTEXT — INCLUDED IN EVERY PROMPT\n",
    "# ==============================================================================\n",
    "\n",
    "_EGYPT_CONTEXT = \"\"\"\n",
    "EGYPT PHARMACEUTICAL MARKET CONTEXT:\n",
    "- Regulator: Egyptian Drug Authority (EDA), formerly NODCAR\n",
    "- Major LOCAL manufacturers (brands may be unknown globally):\n",
    "    EIPICO, Pharco, Eva Pharma, CID, Amriya, Alexandria Pharma,\n",
    "    Minapharm, Nile Pharma, Mash Premiere, Global Napi, MUP (Medical Union Pharmaceuticals),\n",
    "    Rameda, Adwia, Sigma Tec, Egyptian Int'l Pharmaceutical Industries (EIPI),\n",
    "    Delta Pharma, October Pharma, Chemipharm, Arab Drug Company\n",
    "- Multinational brands registered in Egypt:\n",
    "    Glaxo Egypt (GSK), Sanofi Egypt, Novartis Egypt, AstraZeneca Egypt,\n",
    "    Pfizer Egypt, Abbott Egypt, Roche Egypt, Bayer Egypt, Hikma Egypt\n",
    "- Egypt registers BOTH originator brands AND local copies under brand names\n",
    "- Local Egyptian brand name patterns:\n",
    "    - Often end in: -ox, -mox, -cin, -ol, -al, -in, -cil, -ex, -ac, -ix, -cap\n",
    "    - Prefix with manufacturer: Pharco-, Eva-, Nile-, Alex-, Egy-, Sigma-\n",
    "    - Example local brands: Abimol (paracetamol by MUP), Abramox (amoxicillin by Abram),\n",
    "      Acapril (captopril/enalapril), Abrammune (azathioprine), Accord Long (metformin),\n",
    "      Abelia, Abelcet (amphotericin B lipid complex)\n",
    "- Egypt uses BOTH INN names and local brand names in prescriptions\n",
    "- Concentration: Egyptian market commonly uses mg, mg/ml, % in labeling\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GROQ PROMPTS — EGYPT-AWARE (3 ROUNDS)\n",
    "# ==============================================================================\n",
    "\n",
    "GROQ_PROMPT_VALIDATE = _EGYPT_CONTEXT + \"\"\"\n",
    "---\n",
    "You are a senior pharmaceutical expert specializing in the EGYPTIAN drug market.\n",
    "\n",
    "INPUT FORMAT: \"Brand Name [concentration]\"\n",
    "  - Dosage FORM (tabs, caps, syrup…) and pack COUNT already removed.\n",
    "  - Concentration may remain (500mg, 100mg/ml, 0.5%…) — KEEP IT.\n",
    "\n",
    "YOUR TASK:\n",
    "1. Is the NAME a real BRAND/TRADE name registered in EGYPT or by a manufacturer selling in Egypt?\n",
    "2. Generic/INN names (amoxicillin, paracetamol, aripiprazole…) -> flag as is_generic_name\n",
    "3. Fix typos or capitalization -> correct ONLY the name, keep concentration unchanged\n",
    "4. egypt_market: true if confirmed or very likely in Egyptian market; false if not found\n",
    "\n",
    "RULES:\n",
    "- KEEP concentration exactly as-is\n",
    "- DO NOT add dosage form\n",
    "- DO NOT change to generic name\n",
    "- Consider Egyptian local brands (Abimol, Abramox, Acapril…) as valid trade names\n",
    "- If unsure whether it's in Egypt specifically -> set egypt_market: false, confidence < 0.85\n",
    "\n",
    "RESPOND WITH ONLY valid JSON — no markdown:\n",
    "{\n",
    "  \"input\": \"the name you received\",\n",
    "  \"is_tradename\": true or false,\n",
    "  \"is_generic_name\": true or false,\n",
    "  \"egypt_market\": true or false,\n",
    "  \"corrected_tradename\": \"Brand Name [concentration]\" or null,\n",
    "  \"correction_type\": \"none\" | \"capitalization\" | \"typo_fix\" | \"full_correction\" | \"not_a_brand\",\n",
    "  \"correction_note\": \"what changed + egypt market note\" or null,\n",
    "  \"confidence\": 0.0 to 1.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "GROQ_PROMPT_VERIFY = _EGYPT_CONTEXT + \"\"\"\n",
    "---\n",
    "You are a pharmaceutical verification expert for the EGYPTIAN drug market.\n",
    "\n",
    "A previous AI check gave LOW CONFIDENCE or a CORRECTION for this trade name.\n",
    "Do a DEFINITIVE check focused on:\n",
    "  1. Is this brand name ACTUALLY registered in Egypt (EDA database)?\n",
    "  2. Is the spelling correct for the Egyptian market version?\n",
    "  3. Some brands have different names in Egypt vs. globally — use the EGYPTIAN name.\n",
    "\n",
    "RESPOND WITH ONLY valid JSON — no extra text:\n",
    "{\n",
    "  \"input\": \"name being verified\",\n",
    "  \"final_corrected_tradename\": \"correct Egyptian market brand name [concentration]\" or null,\n",
    "  \"is_confirmed_tradename\": true or false,\n",
    "  \"is_confirmed_generic\": true or false,\n",
    "  \"egypt_market\": true or false,\n",
    "  \"verification_note\": \"specific reasoning about Egyptian market presence\",\n",
    "  \"final_confidence\": 0.0 to 1.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "GROQ_PROMPT_EGYPT_CONFIRM = _EGYPT_CONTEXT + \"\"\"\n",
    "---\n",
    "You are an EGYPT DRUG MARKET specialist with deep knowledge of the EDA drug registry.\n",
    "\n",
    "FINAL TASK: Give a definitive YES/NO answer — is this brand name present in the Egyptian market?\n",
    "\n",
    "Check specifically:\n",
    "- EDA (Egyptian Drug Authority) registered products\n",
    "- Egyptian pharmacy formulary / price lists\n",
    "- Products manufactured by Egyptian companies OR imported and registered in Egypt\n",
    "- Include products that may be discontinued but were historically available\n",
    "\n",
    "RESPOND WITH ONLY valid JSON — no extra text:\n",
    "{\n",
    "  \"input\": \"the name\",\n",
    "  \"egypt_market_confirmed\": true or false,\n",
    "  \"final_corrected_tradename\": \"exact name as used in Egypt [concentration]\" or null,\n",
    "  \"manufacturer_in_egypt\": \"company name or null\",\n",
    "  \"generic_name\": \"INN/generic name of the active ingredient\",\n",
    "  \"egypt_confidence\": 0.0 to 1.0,\n",
    "  \"egypt_note\": \"specific evidence for Egyptian market presence or absence\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# KEY ROTATION & RATE LIMIT HANDLING\n",
    "# ==============================================================================\n",
    "\n",
    "_key_cooldown_until = {}\n",
    "\n",
    "\n",
    "def _get_retry_after(resp) -> int:\n",
    "    \"\"\"Compute retry delay from response headers (bounded).\"\"\"\n",
    "    try:\n",
    "        val = resp.headers.get(\"retry-after\") or resp.headers.get(\n",
    "            \"x-ratelimit-reset-requests\", \"60\"\n",
    "        )\n",
    "        return min(int(float(val)), 120)\n",
    "    except Exception:\n",
    "        return 60\n",
    "\n",
    "\n",
    "def rotate_key_if_needed():\n",
    "    \"\"\"Rotate API key index every ROTATE_EVERY processed trade names.\"\"\"\n",
    "    global _current_key_idx, _processed_count\n",
    "    _processed_count += 1\n",
    "    if _processed_count % ROTATE_EVERY == 0:\n",
    "        old = _current_key_idx\n",
    "        _current_key_idx = (_current_key_idx + 1) % len(GROQ_API_KEYS)\n",
    "        log(\n",
    "            f\"Key rotation: key[{old}] -> key[{_current_key_idx}] \"\n",
    "            f\"(after {_processed_count} trade names)\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _call_groq(system_prompt: str, user_message: str, max_retries: int = 2) -> dict | None:\n",
    "    \"\"\"Call Groq with key rotation + cooldown logic, returning parsed JSON dict or None.\"\"\"\n",
    "    global _current_key_idx\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GROQ_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0.05,\n",
    "        \"max_tokens\": 300,\n",
    "    }\n",
    "\n",
    "    n_keys = len(GROQ_API_KEYS)\n",
    "\n",
    "    for attempt in range(n_keys * max_retries):\n",
    "        key_idx = (_current_key_idx + attempt) % n_keys\n",
    "        now = time.time()\n",
    "        cooldown_left = _key_cooldown_until.get(key_idx, 0) - now\n",
    "\n",
    "        if cooldown_left > 0:\n",
    "            if attempt < n_keys - 1:\n",
    "                continue\n",
    "\n",
    "            min_wait = min(\n",
    "                max(_key_cooldown_until.get(i, 0) - now, 0) for i in range(n_keys)\n",
    "            )\n",
    "            log(f\"All keys are on cooldown. Waiting {min_wait:.0f}s\", \"WARN\")\n",
    "            time.sleep(min_wait + 1)\n",
    "            _key_cooldown_until.clear()\n",
    "            continue\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEYS[key_idx]}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = requests.post(GROQ_URL, headers=headers, json=payload, timeout=25)\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                raw = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                return json.loads(raw)\n",
    "\n",
    "            if resp.status_code == 429:\n",
    "                wait = _get_retry_after(resp)\n",
    "                _key_cooldown_until[key_idx] = time.time() + wait\n",
    "                log(f\"HTTP 429 on key[{key_idx}]. Cooldown {wait}s\", \"WARN\")\n",
    "                continue\n",
    "\n",
    "            if resp.status_code == 401:\n",
    "                _key_cooldown_until[key_idx] = time.time() + 7200\n",
    "                log(f\"HTTP 401 on key[{key_idx}]. Disabling for 2 hours\", \"ERROR\")\n",
    "                continue\n",
    "\n",
    "            log(f\"Groq HTTP {resp.status_code}\", \"WARN\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            log(f\"JSON parse error on key[{key_idx}]\", \"WARN\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            log(f\"Timeout on key[{key_idx}]\", \"WARN\")\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            log(f\"Error on key[{key_idx}]: {e}\", \"WARN\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3-ROUND VALIDATION & VERIFICATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def validate_and_verify(tradename: str, groq_delay: float = 0.3) -> dict:\n",
    "    \"\"\"\n",
    "    Three Groq rounds (Egypt-aware):\n",
    "\n",
    "    Round 1 (VALIDATE):\n",
    "      - Is it a real trade name?\n",
    "      - Is it present in the Egyptian market?\n",
    "      - Spelling/capitalization correction\n",
    "\n",
    "    Round 2 (VERIFY) triggers if:\n",
    "      - confidence < threshold\n",
    "      - correction applied\n",
    "      - trade name but Egypt presence not confirmed\n",
    "\n",
    "    Round 3 (EGYPT_CONFIRM) triggers if:\n",
    "      - trade name still not confirmed for Egypt after round 2\n",
    "    \"\"\"\n",
    "    log(f\"  Round 1 — Validate + Egypt check: '{tradename}'\")\n",
    "    r1 = _call_groq(\n",
    "        GROQ_PROMPT_VALIDATE,\n",
    "        f'Validate this drug trade name for the Egyptian market: \"{tradename}\"',\n",
    "    )\n",
    "    time.sleep(groq_delay)\n",
    "\n",
    "    if r1 is None:\n",
    "        log(\"  Round 1 failed. Falling back\", \"WARN\")\n",
    "        return _fallback(tradename)\n",
    "\n",
    "    conf1 = r1.get(\"confidence\", 0.0)\n",
    "    corrected1 = r1.get(\"corrected_tradename\") or tradename\n",
    "    is_brand = r1.get(\"is_tradename\", False)\n",
    "    is_generic = r1.get(\"is_generic_name\", False)\n",
    "    egypt1 = r1.get(\"egypt_market\", False)\n",
    "    corr_type = r1.get(\"correction_type\", \"none\")\n",
    "\n",
    "    log(\n",
    "        f\"  Round 1 -> '{corrected1}' | conf={conf1:.2f} | \"\n",
    "        f\"egypt={'YES' if egypt1 else 'NO'} | type={corr_type}\"\n",
    "    )\n",
    "\n",
    "    needs_round2 = (\n",
    "        conf1 < CONFIDENCE_THRESHOLD\n",
    "        or corr_type in (\"typo_fix\", \"full_correction\")\n",
    "        or (is_brand and not egypt1)\n",
    "        or (is_brand and not corrected1)\n",
    "    )\n",
    "\n",
    "    if not needs_round2:\n",
    "        log(f\"  Confirmed in 1 round. Egypt: {'YES' if egypt1 else 'NO'}\")\n",
    "        return _build(\n",
    "            tradename,\n",
    "            corrected1,\n",
    "            is_brand,\n",
    "            is_generic,\n",
    "            conf1,\n",
    "            r1.get(\"correction_note\"),\n",
    "            egypt1,\n",
    "            None,\n",
    "            True,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    log(f\"  Round 2 — Verify + Egypt: '{corrected1}' (conf={conf1:.2f})\")\n",
    "    r2 = _call_groq(\n",
    "        GROQ_PROMPT_VERIFY,\n",
    "        (\n",
    "            \"Verify this drug trade name for the Egyptian market:\\n\"\n",
    "            f'Original input: \"{tradename}\"\\n'\n",
    "            f'Previous suggestion: \"{corrected1}\" (confidence {conf1:.2f})\\n'\n",
    "            f\"Previous egypt_market: {egypt1}\\n\"\n",
    "            \"Give definitive answer.\"\n",
    "        ),\n",
    "    )\n",
    "    time.sleep(groq_delay)\n",
    "\n",
    "    if r2 is None:\n",
    "        log(\"  Round 2 failed. Using Round 1 results\", \"WARN\")\n",
    "        return _build(\n",
    "            tradename,\n",
    "            corrected1,\n",
    "            is_brand,\n",
    "            is_generic,\n",
    "            conf1,\n",
    "            r1.get(\"correction_note\"),\n",
    "            egypt1,\n",
    "            None,\n",
    "            False,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    corrected2 = r2.get(\"final_corrected_tradename\") or corrected1\n",
    "    conf2 = r2.get(\"final_confidence\", conf1)\n",
    "    egypt2 = r2.get(\"egypt_market\", egypt1)\n",
    "    is_brand2 = r2.get(\"is_confirmed_tradename\", is_brand)\n",
    "    is_gen2 = r2.get(\"is_confirmed_generic\", is_generic)\n",
    "    note2 = r2.get(\"verification_note\") or r1.get(\"correction_note\")\n",
    "\n",
    "    log(f\"  Round 2 -> '{corrected2}' | conf={conf2:.2f} | egypt={'YES' if egypt2 else 'NO'}\")\n",
    "\n",
    "    needs_round3 = is_brand2 and (not egypt2 or conf2 < CONFIDENCE_THRESHOLD)\n",
    "\n",
    "    if not needs_round3:\n",
    "        log(f\"  Confirmed in 2 rounds. Egypt: {'YES' if egypt2 else 'NO'}\")\n",
    "        return _build(\n",
    "            tradename,\n",
    "            corrected2,\n",
    "            is_brand2,\n",
    "            is_gen2,\n",
    "            conf2,\n",
    "            note2,\n",
    "            egypt2,\n",
    "            None,\n",
    "            True,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    log(f\"  Round 3 — Egypt Market Confirm: '{corrected2}'\")\n",
    "    r3 = _call_groq(\n",
    "        GROQ_PROMPT_EGYPT_CONFIRM,\n",
    "        f'Confirm Egyptian market presence for: \"{corrected2}\"',\n",
    "    )\n",
    "    time.sleep(groq_delay)\n",
    "\n",
    "    if r3 is None:\n",
    "        log(\"  Round 3 failed. Using Round 2 results\", \"WARN\")\n",
    "        return _build(\n",
    "            tradename,\n",
    "            corrected2,\n",
    "            is_brand2,\n",
    "            is_gen2,\n",
    "            conf2,\n",
    "            note2,\n",
    "            egypt2,\n",
    "            None,\n",
    "            True,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    egypt3 = r3.get(\"egypt_market_confirmed\", egypt2)\n",
    "    corrected3 = r3.get(\"final_corrected_tradename\") or corrected2\n",
    "    conf3 = r3.get(\"egypt_confidence\", conf2)\n",
    "    manufacturer = r3.get(\"manufacturer_in_egypt\")\n",
    "    generic_name = r3.get(\"generic_name\")\n",
    "    note3 = r3.get(\"egypt_note\") or note2\n",
    "\n",
    "    log(\n",
    "        f\"  Round 3 -> Egypt={'YES' if egypt3 else 'NO'} | \"\n",
    "        f\"conf={conf3:.2f} | manufacturer={manufacturer or '-'}\"\n",
    "    )\n",
    "\n",
    "    return _build(\n",
    "        tradename,\n",
    "        corrected3,\n",
    "        is_brand2,\n",
    "        is_gen2,\n",
    "        conf3,\n",
    "        note3,\n",
    "        egypt3,\n",
    "        manufacturer,\n",
    "        True,\n",
    "        3,\n",
    "        generic_name=generic_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def _build(\n",
    "    original,\n",
    "    corrected,\n",
    "    is_brand,\n",
    "    is_generic,\n",
    "    confidence,\n",
    "    note,\n",
    "    egypt_market,\n",
    "    egypt_manufacturer,\n",
    "    verified,\n",
    "    rounds,\n",
    "    generic_name=None,\n",
    ") -> dict:\n",
    "    \"\"\"Build the standardized validation output dict.\"\"\"\n",
    "    return {\n",
    "        \"original\": original,\n",
    "        \"corrected\": corrected if is_brand else None,\n",
    "        \"is_tradename\": is_brand,\n",
    "        \"is_generic\": is_generic,\n",
    "        \"confidence\": round(confidence, 3),\n",
    "        \"note\": note,\n",
    "        \"egypt_market\": egypt_market,\n",
    "        \"egypt_manufacturer\": egypt_manufacturer,\n",
    "        \"generic_name\": generic_name,\n",
    "        \"verified\": verified,\n",
    "        \"groq_rounds\": rounds,\n",
    "    }\n",
    "\n",
    "\n",
    "def _fallback(tradename: str) -> dict:\n",
    "    \"\"\"Fallback when Groq is unavailable.\"\"\"\n",
    "    return {\n",
    "        \"original\": tradename,\n",
    "        \"corrected\": tradename,\n",
    "        \"is_tradename\": True,\n",
    "        \"is_generic\": False,\n",
    "        \"confidence\": 0.0,\n",
    "        \"note\": \"Groq unavailable — kept as-is\",\n",
    "        \"egypt_market\": False,\n",
    "        \"egypt_manufacturer\": None,\n",
    "        \"generic_name\": None,\n",
    "        \"verified\": False,\n",
    "        \"groq_rounds\": 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PROGRESS IO\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def load_progress(filepath: str | None = None) -> dict:\n",
    "    \"\"\"Load progress JSON if it exists; return empty dict otherwise.\"\"\"\n",
    "    path = filepath or PROGRESS_FILE\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_progress(progress: dict, filepath: str | None = None):\n",
    "    \"\"\"Save progress JSON.\"\"\"\n",
    "    path = filepath or PROGRESS_FILE\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CORE — APPEND-ONLY CSV LOGIC (CONFIRMED ROWS ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "NEW_COLS = [\n",
    "    \"tradename_cleaned\",\n",
    "    \"tradename_corrected\",\n",
    "    \"tradename_is_valid\",\n",
    "    \"tradename_is_generic\",\n",
    "    \"tradename_confidence\",\n",
    "    \"tradename_correction_note\",\n",
    "    \"tradename_egypt_market\",\n",
    "    \"tradename_egypt_manufacturer\",\n",
    "    \"tradename_generic_name\",\n",
    "    \"tradename_verified\",\n",
    "    \"tradename_groq_rounds\",\n",
    "]\n",
    "\n",
    "\n",
    "def _build_confirmed_rows(\n",
    "    df_original: pd.DataFrame,\n",
    "    tradename_col: str,\n",
    "    batch_validations: dict,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Build a DataFrame of rows from df_original corresponding to this batch's\n",
    "    confirmed trade names (is_tradename=True), preserving df_original order.\n",
    "\n",
    "    batch_validations: { cleaned_tradename: validation_dict }\n",
    "    \"\"\"\n",
    "    if not batch_validations:\n",
    "        return None\n",
    "\n",
    "    confirmed = {k: v for k, v in batch_validations.items() if v.get(\"is_tradename\", False)}\n",
    "    if not confirmed:\n",
    "        return None\n",
    "\n",
    "    mask = df_original[\"_tradename_clean\"].isin(confirmed.keys())\n",
    "    df_batch = df_original[mask].copy()\n",
    "\n",
    "    if df_batch.empty:\n",
    "        return None\n",
    "\n",
    "    def _get(row, field, default=\"\"):\n",
    "        v = confirmed.get(row[\"_tradename_clean\"], {})\n",
    "        return v.get(field, default)\n",
    "\n",
    "    df_batch[\"tradename_cleaned\"] = df_batch[\"_tradename_clean\"]\n",
    "    df_batch[\"tradename_corrected\"] = df_batch.apply(\n",
    "        lambda r: confirmed.get(r[\"_tradename_clean\"], {}).get(\"corrected\") or r[\"_tradename_clean\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    df_batch[\"tradename_is_valid\"] = df_batch.apply(lambda r: _get(r, \"is_tradename\", True), axis=1)\n",
    "    df_batch[\"tradename_is_generic\"] = df_batch.apply(lambda r: _get(r, \"is_generic\", False), axis=1)\n",
    "    df_batch[\"tradename_confidence\"] = df_batch.apply(lambda r: _get(r, \"confidence\", 0.0), axis=1)\n",
    "    df_batch[\"tradename_correction_note\"] = df_batch.apply(lambda r: _get(r, \"note\", \"\"), axis=1)\n",
    "\n",
    "    df_batch[\"tradename_egypt_market\"] = df_batch.apply(\n",
    "        lambda r: _get(r, \"egypt_market\", False), axis=1\n",
    "    )\n",
    "    df_batch[\"tradename_egypt_manufacturer\"] = df_batch.apply(\n",
    "        lambda r: _get(r, \"egypt_manufacturer\", \"\"), axis=1\n",
    "    )\n",
    "    df_batch[\"tradename_generic_name\"] = df_batch.apply(\n",
    "        lambda r: _get(r, \"generic_name\", \"\"), axis=1\n",
    "    )\n",
    "\n",
    "    df_batch[\"tradename_verified\"] = df_batch.apply(lambda r: _get(r, \"verified\", False), axis=1)\n",
    "    df_batch[\"tradename_groq_rounds\"] = df_batch.apply(lambda r: _get(r, \"groq_rounds\", 0), axis=1)\n",
    "\n",
    "    df_batch.drop(columns=[\"_tradename_clean\"], inplace=True, errors=\"ignore\")\n",
    "    return df_batch\n",
    "\n",
    "\n",
    "def append_confirmed_to_csv(\n",
    "    df_original: pd.DataFrame,\n",
    "    tradename_col: str,\n",
    "    batch_validations: dict,\n",
    "    is_first_write: bool,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Append confirmed rows from this batch to OUTPUT_CSV.\n",
    "\n",
    "    - is_first_write=True  -> write with header (first time or after reset)\n",
    "    - is_first_write=False -> append without header\n",
    "\n",
    "    Returns number of appended rows.\n",
    "    \"\"\"\n",
    "    df_batch = _build_confirmed_rows(df_original, tradename_col, batch_validations)\n",
    "    if df_batch is None or df_batch.empty:\n",
    "        return 0\n",
    "\n",
    "    write_header = is_first_write or not os.path.exists(OUTPUT_CSV)\n",
    "\n",
    "    df_batch.to_csv(\n",
    "        OUTPUT_CSV,\n",
    "        mode=\"w\" if write_header else \"a\",\n",
    "        header=write_header,\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\",\n",
    "    )\n",
    "    return len(df_batch)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ORDER GUARANTEE — ORDERED UNIQUE LIST BY FIRST APPEARANCE\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def get_ordered_unique(df: pd.DataFrame, clean_col: str = \"_tradename_clean\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of unique trade names preserving the order of first appearance\n",
    "    in the DataFrame (NOT sorted alphabetically).\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for val in df[clean_col].dropna():\n",
    "        if val and len(val) > 1 and val not in seen:\n",
    "            seen.add(val)\n",
    "            result.append(val)\n",
    "    return result\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# JSON EXPORT + SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def save_json(results: dict):\n",
    "    \"\"\"Write a compact validated JSON output (done entries only).\"\"\"\n",
    "    json_out = {}\n",
    "    for tn, data in results.items():\n",
    "        if data.get(\"status\") != \"done\":\n",
    "            continue\n",
    "\n",
    "        v = data.get(\"validation\", {})\n",
    "        json_out[tn] = {\n",
    "            \"original\": tn,\n",
    "            \"corrected\": v.get(\"corrected\"),\n",
    "            \"is_tradename\": v.get(\"is_tradename\", False),\n",
    "            \"is_generic\": v.get(\"is_generic\", False),\n",
    "            \"confidence\": v.get(\"confidence\", 0.0),\n",
    "            \"correction_note\": v.get(\"note\"),\n",
    "            \"egypt_market\": v.get(\"egypt_market\", False),\n",
    "            \"egypt_manufacturer\": v.get(\"egypt_manufacturer\", None),\n",
    "            \"generic_name\": v.get(\"generic_name\", None),\n",
    "            \"verified\": v.get(\"verified\", False),\n",
    "            \"groq_rounds\": v.get(\"groq_rounds\", 0),\n",
    "        }\n",
    "\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    log(f\"JSON saved: {OUTPUT_JSON} ({len(json_out):,} entries)\")\n",
    "    return json_out\n",
    "\n",
    "\n",
    "def print_summary(results: dict):\n",
    "    \"\"\"Print a pipeline summary based on `results`.\"\"\"\n",
    "    done = [v for v in results.values() if v.get(\"status\") == \"done\"]\n",
    "    total = len(done)\n",
    "    valid = sum(1 for v in done if (v.get(\"validation\") or {}).get(\"is_tradename\"))\n",
    "    generics = sum(1 for v in done if (v.get(\"validation\") or {}).get(\"is_generic\"))\n",
    "    corrected = sum(\n",
    "        1\n",
    "        for v in done\n",
    "        if (v.get(\"validation\") or {}).get(\"note\")\n",
    "        and (v.get(\"validation\") or {}).get(\"is_tradename\")\n",
    "    )\n",
    "    egypt_yes = sum(1 for v in done if (v.get(\"validation\") or {}).get(\"egypt_market\"))\n",
    "    egypt_no = valid - egypt_yes\n",
    "    rounds_3 = sum(1 for v in done if (v.get(\"validation\") or {}).get(\"groq_rounds\", 0) >= 3)\n",
    "    rounds_2 = sum(1 for v in done if (v.get(\"validation\") or {}).get(\"groq_rounds\", 0) == 2)\n",
    "\n",
    "    log(\"=\" * 70)\n",
    "    log(\"Pipeline Summary:\")\n",
    "    log(f\"   Total processed                       : {total:,}\")\n",
    "    log(f\"   Valid trade names                     : {valid:,}  (included in CSV)\")\n",
    "    log(f\"      Confirmed in Egypt market          : {egypt_yes:,}\")\n",
    "    log(f\"      Not confirmed in Egypt market      : {egypt_no:,}\")\n",
    "    log(f\"   Spelling corrections applied          : {corrected:,}\")\n",
    "    log(f\"   Verified via Round 3 (Egypt Confirm)  : {rounds_3:,}\")\n",
    "    log(f\"   Verified via Round 2 (Verify)         : {rounds_2:,}\")\n",
    "    log(f\"   Generic names flagged                 : {generics:,}  (not included in CSV)\")\n",
    "    log(f\"   Unknown                               : {total - valid - generics:,}  (not included in CSV)\")\n",
    "    log(f\"   CSV Output                            : {OUTPUT_CSV}\")\n",
    "    log(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def run_full_pipeline(\n",
    "    input_csv: str | None = None,\n",
    "    groq_delay: float = 0.3,\n",
    "    save_every: int = 10,\n",
    "    max_tradenames: int | None = None,\n",
    "    start_from: int | None = None,\n",
    "    end_at: int | None = None,\n",
    "    worker_name: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline v5 — append-only CSV with confirmed rows only, preserving input order.\n",
    "\n",
    "    Unique list order = first appearance order in df_original.\n",
    "    If start_from/end_at are provided, each worker processes a range of that same list.\n",
    "    \"\"\"\n",
    "    if input_csv is None:\n",
    "        input_csv = INPUT_CSV\n",
    "\n",
    "    log(\"=\" * 65)\n",
    "    tag = f\" [{worker_name}]\" if worker_name else \"\"\n",
    "    log(f\"TRADENAME PIPELINE v5{tag}\")\n",
    "    log(f\"save_every={save_every} | append-only confirmed rows\")\n",
    "    log(\"=\" * 65)\n",
    "\n",
    "    log(f\"Reading CSV: {input_csv}\")\n",
    "    df_original = pd.read_csv(input_csv)\n",
    "    log(f\"Rows: {len(df_original):,} | Columns: {len(df_original.columns):,}\")\n",
    "\n",
    "    tradename_col = None\n",
    "    for c in [\n",
    "        \"Tradename\",\n",
    "        \"tradename\",\n",
    "        \"TradeName\",\n",
    "        \"trade_name\",\n",
    "        \"BrandName\",\n",
    "        \"brandname\",\n",
    "        \"brand_name\",\n",
    "        \"DrugName\",\n",
    "    ]:\n",
    "        if c in df_original.columns:\n",
    "            tradename_col = c\n",
    "            break\n",
    "\n",
    "    if tradename_col is None:\n",
    "        raise ValueError(\n",
    "            \"Tradename column not found.\\n\"\n",
    "            f\"Available columns: {list(df_original.columns)}\"\n",
    "        )\n",
    "\n",
    "    log(f\"Tradename column: '{tradename_col}'\")\n",
    "\n",
    "    df_original[\"_tradename_clean\"] = df_original[tradename_col].apply(clean_tradename)\n",
    "\n",
    "    ordered_unique = get_ordered_unique(df_original, \"_tradename_clean\")\n",
    "    total_all = len(ordered_unique)\n",
    "\n",
    "    log(f\"Unique trade names (df_original order): {total_all:,}\")\n",
    "    log(f\"First 5: {ordered_unique[:5]}\")\n",
    "    log(f\"Last 5: {ordered_unique[-5:]}\")\n",
    "\n",
    "    if max_tradenames:\n",
    "        work_list = ordered_unique[:max_tradenames]\n",
    "        log(f\"Testing mode: first {max_tradenames}\")\n",
    "    else:\n",
    "        s = (start_from - 1) if start_from else 0\n",
    "        e = end_at if end_at else total_all\n",
    "        work_list = ordered_unique[s:e]\n",
    "        log(f\"Range: #{s + 1:,} -> #{e:,} ({len(work_list):,} unique names)\")\n",
    "        log(f\"First 3 in range: {work_list[:3]}\")\n",
    "\n",
    "    if start_from and not max_tradenames:\n",
    "        worker_progress = PROGRESS_FILE.replace(\n",
    "            \".json\", f\"_part{start_from}_{end_at or total_all}.json\"\n",
    "        )\n",
    "    else:\n",
    "        worker_progress = PROGRESS_FILE\n",
    "\n",
    "    log(f\"Progress file: {worker_progress}\")\n",
    "\n",
    "    progress = load_progress(worker_progress)\n",
    "    done_before = sum(1 for v in progress.values() if v.get(\"status\") == \"done\")\n",
    "    if done_before:\n",
    "        log(f\"Resuming: {done_before:,} unique names already completed\")\n",
    "\n",
    "    results = {k: v for k, v in progress.items() if v.get(\"status\") == \"done\"}\n",
    "    total = len(work_list)\n",
    "    processed = 0\n",
    "\n",
    "    is_first_write = (done_before == 0) or (not os.path.exists(OUTPUT_CSV))\n",
    "    batch_validations = {}\n",
    "\n",
    "    for idx, tradename in enumerate(work_list):\n",
    "        if tradename in results:\n",
    "            continue\n",
    "\n",
    "        log(f\"\\n[{idx + 1}/{total}] '{tradename}'\")\n",
    "\n",
    "        validation = validate_and_verify(tradename, groq_delay=groq_delay)\n",
    "\n",
    "        if validation[\"is_tradename\"]:\n",
    "            log(\n",
    "                f\"  BRAND -> '{validation['corrected']}' \"\n",
    "                f\"conf={validation['confidence']:.2f} \"\n",
    "                f\"rounds={validation['groq_rounds']}\"\n",
    "            )\n",
    "        elif validation[\"is_generic\"]:\n",
    "            log(\"  GENERIC -> not appended to CSV\")\n",
    "        else:\n",
    "            log(f\"  UNKNOWN conf={validation['confidence']:.2f} -> not appended to CSV\")\n",
    "\n",
    "        results[tradename] = {\"status\": \"done\", \"tradename\": tradename, \"validation\": validation}\n",
    "\n",
    "        if validation[\"is_tradename\"]:\n",
    "            batch_validations[tradename] = validation\n",
    "\n",
    "        processed += 1\n",
    "        rotate_key_if_needed()\n",
    "\n",
    "        if processed % save_every == 0:\n",
    "            save_progress(results, worker_progress)\n",
    "\n",
    "            added = append_confirmed_to_csv(\n",
    "                df_original,\n",
    "                tradename_col,\n",
    "                batch_validations,\n",
    "                is_first_write,\n",
    "            )\n",
    "\n",
    "            log(\n",
    "                f\"Batch checkpoint: +{added:,} rows appended to CSV | \"\n",
    "                f\"progress: {processed}/{total}\"\n",
    "            )\n",
    "\n",
    "            if added > 0:\n",
    "                is_first_write = False\n",
    "\n",
    "            batch_validations = {}\n",
    "\n",
    "    if batch_validations:\n",
    "        save_progress(results, worker_progress)\n",
    "        added = append_confirmed_to_csv(\n",
    "            df_original,\n",
    "            tradename_col,\n",
    "            batch_validations,\n",
    "            is_first_write,\n",
    "        )\n",
    "        log(f\"\\nFinal batch: +{added:,} rows appended to CSV\")\n",
    "        if added > 0:\n",
    "            is_first_write = False\n",
    "    else:\n",
    "        save_progress(results, worker_progress)\n",
    "\n",
    "    save_json(results)\n",
    "    print_summary(results)\n",
    "\n",
    "    df_original.drop(columns=[\"_tradename_clean\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    log(f\"\\nPipeline complete. New processed unique names: {processed}\")\n",
    "    log(f\"CSV Output: {OUTPUT_CSV}\")\n",
    "    return df_original, results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TEST HELPERS\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def test_clean(samples: list[str] | None = None):\n",
    "    \"\"\"Test clean_tradename() without API calls.\"\"\"\n",
    "    if samples is None:\n",
    "        samples = [\n",
    "            (\"1 2 3 (one Two Three) Syrup 120 Ml\", \"\"),\n",
    "            (\"1 2 3 (one Two Three) 20 F.c.tabs.\", \"\"),\n",
    "            (\"A1 Cream 100 Gm\", \"A1\"),\n",
    "            (\"Abelcet\", \"Abelcet\"),\n",
    "            (\"Aripiprazole\", \"Aripiprazole\"),\n",
    "            (\"Abilify 15mg 30 F.c.tabs.\", \"Abilify 15mg\"),\n",
    "            (\"Abilia 15mg 30 F.c.tabs.\", \"Abilia 15mg\"),\n",
    "            (\"Abimol Extra 20\", \"Abimol Extra\"),\n",
    "            (\"Abimol\", \"Abimol\"),\n",
    "            (\"Abrammune 50 Mg 10 Caps.(n/a Yet)\", \"Abrammune 50mg\"),\n",
    "            (\"Abramox 100mg/ml Syrup\", \"Abramox 100mg/ml\"),\n",
    "            (\"Abramox 50mg/ml Syrup\", \"Abramox 50mg/ml\"),\n",
    "            (\"Accord Long 600mg 5 Effervescent Tablets\", \"Accord Long 600mg\"),\n",
    "            (\"Acetaminophen 125mg Paed. Supp.u.s.p.xxiii\", \"Acetaminophen 125mg\"),\n",
    "            (\"Acetaminophen 500mg 20 Tab\", \"Acetaminophen 500mg\"),\n",
    "            (\"Acc 200mg 20 Sachets\", \"Acc 200mg\"),\n",
    "            (\"Acapril\", \"Acapril\"),\n",
    "            (\"Augmentin 625mg 14 Tabs\", \"Augmentin 625mg\"),\n",
    "            (\"Panadol 500mg 24 Tablets\", \"Panadol 500mg\"),\n",
    "            (\"Lipitor 20mg 30 F.C.Tabs\", \"Lipitor 20mg\"),\n",
    "            (\"Voltaren 1% Gel 100gm\", \"Voltaren 1%\"),\n",
    "            (\"HCl 0.9% 100ml Solution\", \"Hcl 0.9%\"),\n",
    "            (\"Amoxil 250mg/5ml Suspension\", \"Amoxil 250mg/5ml\"),\n",
    "            (\"Zithromax 500mg 3 Caps\", \"Zithromax 500mg\"),\n",
    "        ]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 75)\n",
    "    print(\"clean_tradename() test | PASS=OK | FAIL=Mismatch\")\n",
    "    print(f\"{'INPUT':<45} {'EXPECTED':<22} {'GOT':<22} STATUS\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "    passed = failed = 0\n",
    "    for raw, expected in samples:\n",
    "        got = clean_tradename(raw)\n",
    "        ok = got == expected\n",
    "        status = \"PASS\" if ok else \"FAIL\"\n",
    "        passed += int(ok)\n",
    "        failed += int(not ok)\n",
    "        print(f\"  {status:<4}  {raw:<43} -> {got!r}\")\n",
    "\n",
    "    print(f\"\\nResult: {passed} pass | {failed} fail out of {passed + failed}\")\n",
    "\n",
    "\n",
    "def test_order(input_csv: str | None = None):\n",
    "    \"\"\"Print ordered unique list (first-appearance order) to verify ordering pre-run.\"\"\"\n",
    "    csv = input_csv or INPUT_CSV\n",
    "    df = pd.read_csv(csv)\n",
    "    col = next(\n",
    "        (\n",
    "            c\n",
    "            for c in [\n",
    "                \"Tradename\",\n",
    "                \"tradename\",\n",
    "                \"TradeName\",\n",
    "                \"trade_name\",\n",
    "                \"BrandName\",\n",
    "                \"brandname\",\n",
    "                \"brand_name\",\n",
    "                \"DrugName\",\n",
    "            ]\n",
    "            if c in df.columns\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    if not col:\n",
    "        print(\"Tradename column not found\")\n",
    "        return\n",
    "\n",
    "    df[\"_tradename_clean\"] = df[col].apply(clean_tradename)\n",
    "    ordered = get_ordered_unique(df, \"_tradename_clean\")\n",
    "\n",
    "    print(f\"\\nOrdered unique list ({len(ordered):,} items)\")\n",
    "    print(f\"First 10: {ordered[:10]}\")\n",
    "    print(f\"Last 10:  {ordered[-10:]}\")\n",
    "\n",
    "    print(\"\\nSuggested split across 4 workers:\")\n",
    "    n = len(ordered)\n",
    "    q = n // 4\n",
    "    print(f\"Worker 1: start_from=1,           end_at={q}\")\n",
    "    print(f\"Worker 2: start_from={q + 1},     end_at={2 * q}\")\n",
    "    print(f\"Worker 3: start_from={2 * q + 1}, end_at={3 * q}\")\n",
    "    print(f\"Worker 4: start_from={3 * q + 1}, end_at={n}\")\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def test_single(tradename: str, delay: float = 0.5):\n",
    "    \"\"\"Run full validation on a single tradename.\"\"\"\n",
    "    print(f\"\\nTesting: '{tradename}'\")\n",
    "    cleaned = clean_tradename(tradename)\n",
    "    print(f\"Cleaned: '{cleaned}'\")\n",
    "    if not cleaned:\n",
    "        print(\"Cleaned result is empty\")\n",
    "        return None\n",
    "    result = validate_and_verify(cleaned, groq_delay=delay)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    return result\n",
    "\n",
    "\n",
    "def test_sample(n: int = 5, delay: float = 1.0):\n",
    "    \"\"\"Run pipeline on first N unique names.\"\"\"\n",
    "    return run_full_pipeline(max_tradenames=n, groq_delay=delay, save_every=2)\n",
    "\n",
    "\n",
    "def show_csv_stats():\n",
    "    \"\"\"Show basic stats for the current OUTPUT_CSV.\"\"\"\n",
    "    if not os.path.exists(OUTPUT_CSV):\n",
    "        print(\"No saved CSV yet.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    total = len(df)\n",
    "    verified_2 = (\n",
    "        (df.get(\"tradename_groq_rounds\", pd.Series(dtype=int)) >= 2).sum()\n",
    "        if \"tradename_groq_rounds\" in df.columns\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCSV Stats — {OUTPUT_CSV}\")\n",
    "    print(f\"Total rows          : {total:,}\")\n",
    "    print(f\"Verified >= 2 rounds: {verified_2:,}\")\n",
    "    print(f\"Columns             : {list(df.columns)}\")\n",
    "\n",
    "    if \"tradename_corrected\" in df.columns:\n",
    "        changed = (df[\"tradename_corrected\"] != df.get(\"tradename_cleaned\", \"\")).sum()\n",
    "        print(f\"Changed names       : {changed:,}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MERGE — AFTER ALL WORKERS FINISH\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def merge_all_workers(input_csv: str | None = None):\n",
    "    \"\"\"\n",
    "    After all workers finish:\n",
    "      1) Merge all worker progress JSON files\n",
    "      2) Build final OUTPUT_CSV in correct df_original order (single rebuild)\n",
    "    \"\"\"\n",
    "    pattern = PROGRESS_FILE.replace(\".json\", \"_part*.json\")\n",
    "    part_files = sorted(glob.glob(pattern))\n",
    "    all_files = part_files + ([PROGRESS_FILE] if os.path.exists(PROGRESS_FILE) else [])\n",
    "\n",
    "    if not all_files:\n",
    "        log(\"No progress files found\", \"ERROR\")\n",
    "        return\n",
    "\n",
    "    log(f\"Merging {len(all_files)} progress files\")\n",
    "    merged = {}\n",
    "\n",
    "    for path in all_files:\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            before = len(merged)\n",
    "            merged.update(data)\n",
    "            log(f\"Loaded {os.path.basename(path)}: {len(data):,} (+{len(merged) - before:,})\")\n",
    "        except Exception as e:\n",
    "            log(f\"Failed to load {path}: {e}\", \"ERROR\")\n",
    "\n",
    "    save_progress(merged)\n",
    "    log(f\"Total after merge: {len(merged):,}\")\n",
    "\n",
    "    csv_path = input_csv or INPUT_CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    col = next(\n",
    "        (\n",
    "            c\n",
    "            for c in [\n",
    "                \"Tradename\",\n",
    "                \"tradename\",\n",
    "                \"TradeName\",\n",
    "                \"trade_name\",\n",
    "                \"BrandName\",\n",
    "                \"brandname\",\n",
    "                \"brand_name\",\n",
    "                \"DrugName\",\n",
    "            ]\n",
    "            if c in df.columns\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    if not col:\n",
    "        log(\"Tradename column not found\", \"ERROR\")\n",
    "        return\n",
    "\n",
    "    df[\"_tradename_clean\"] = df[col].apply(clean_tradename)\n",
    "\n",
    "    all_confirmed = {\n",
    "        tn: data[\"validation\"]\n",
    "        for tn, data in merged.items()\n",
    "        if data.get(\"status\") == \"done\" and (data.get(\"validation\") or {}).get(\"is_tradename\", False)\n",
    "    }\n",
    "\n",
    "    log(f\"Confirmed trade names: {len(all_confirmed):,}\")\n",
    "\n",
    "    added = append_confirmed_to_csv(df, col, all_confirmed, is_first_write=True)\n",
    "    log(f\"Final CSV rebuilt: {OUTPUT_CSV} ({added:,} rows)\")\n",
    "\n",
    "    save_json(merged)\n",
    "    print_summary(merged)\n",
    "    log(\"Merge complete\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ENTRY POINT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_pipeline(\n",
    "        start_from=2000,\n",
    "        end_at=3999,\n",
    "        worker_name=\"User\",\n",
    "        save_every=10,\n",
    "        groq_delay=0.3,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
