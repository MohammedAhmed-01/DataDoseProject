{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21181c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - Google Colab Version\n",
    "# =============================================================================\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Set the base project folder path\n",
    "BASE_DIR = '/content/drive/MyDrive/DataDoseDepi'\n",
    "\n",
    "INPUT_FILE  = os.path.join(BASE_DIR, 'DataDoseDataset.csv')\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, 'DataDoseDataset_FinalV.csv')\n",
    "LOG_FILE    = os.path.join(BASE_DIR, 'cleaning_logFinal.txt')\n",
    "\n",
    "# Optional: verify paths before running\n",
    "# print(os.listdir('/content/drive/MyDrive'))\n",
    "# print(os.listdir(BASE_DIR))\n",
    "\n",
    "# =============================================================================\n",
    "# P0 — Immutable Principles\n",
    "# P0.1: Synonym merging is FORBIDDEN (paracetamol ↔ acetaminophen must stay as-is)\n",
    "# P0.2: Duplicate rows across the dataset are allowed; only intra-row deduplication\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# Encoded Token Decoder\n",
    "# Handles __INGxxxx__ placeholders left by upstream encoding corruption.\n",
    "# Add known mappings here as you discover them from your data.\n",
    "# =============================================================================\n",
    "ENCODED_TOKEN_MAP = {\n",
    "    \"__ING0024__\": \"vita\",   # __ING0024__mins       → vitamins (filtered later)\n",
    "    \"__ING0035__\": \"\",       # __ING0035__2          → bare \"2\" (filtered later)\n",
    "    \"__ING0055__\": \"iron\",   # sp__ING0055__olactone → spironolactone\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Garbage Terms (Exact-match only — no substring matching)\n",
    "# =============================================================================\n",
    "GARBAGE_LIST = [\n",
    "    \"invalid\", \"test\", \"unknown\", \"no active ingredient\", \"pending\", \"deleted\",\n",
    "    \"n/a\", \"not available\", \"natural source\", \"mixed\",\n",
    "    \"coming soon\", \"special\", \"bitten\", \"mental\",\n",
    "    \"herbal formula\", \"regurgitation milk formula\", \"gasmin odor\",\n",
    "    \"ethyhexyl\", \"stearly alcohol\", \"silicones\",\n",
    "    \"glycerol stearate\", \"octadeceny ammonium\",\n",
    "    \"distearoylethyl hydroxyethylmonium methosulfate\",\n",
    "    \"capramidopropylbetaine\", \"capryl\", \"cocoamidopropyl betaine\",\n",
    "    \"water\", \"aqua\",\n",
    "    \"dr ey t\", \"dr ey\",\n",
    "    \"gereinigter honig\", \"selected theraputically active\",\n",
    "    \"theraputically active\", \"gereinigter\", \"honig\",\n",
    "    \"selected theraputically\",\n",
    "    \"vitamins\", \"vita\", \"350m\",\n",
    "]\n",
    "GARBAGE_EXACT = set(x.strip().lower() for x in GARBAGE_LIST)\n",
    "\n",
    "# Short tokens that are garbage ONLY when they appear as a complete standalone token\n",
    "GARBAGE_TOKENS_EXACT = {\n",
    "    \"na\", \"n/a\", \"amin\", \"amins\", \"type\", \"formula\",\n",
    "    \"other\", \"high\", \"pre\", \"mixed\", \"special\",\n",
    "    \"as\", \"ivay\", \"potat\", \"len\",\n",
    "    \"2\",\n",
    "    \"vitamin\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# NON-DRUG TOKENS — Supplements/marketing terms that are NOT active ingredients\n",
    "# These are removed as individual tokens (not whole-row deletion).\n",
    "# The remaining valid tokens in the same row are kept.\n",
    "# =============================================================================\n",
    "NON_DRUG_TOKENS = {\n",
    "    \"q10\", \"coq10\", \"q 10\",\n",
    "    \"royal jelly\", \"propolis\", \"bee pollen\", \"bee wax\",\n",
    "    \"antioxidants\", \"antioxidant\",\n",
    "    \"green tea extract\", \"grape seed extract\", \"pine bark extract\",\n",
    "    \"ginkgo biloba\", \"ginseng\",\n",
    "    \"honey\", \"beeswax\", \"aloe vera\", \"aloe\",\n",
    "    \"herbal extract\", \"plant extract\", \"natural extract\",\n",
    "    \"amino acids blend\", \"protein blend\", \"mineral blend\",\n",
    "    \"turmeric\", \"curcumin\", \"ginger\", \"ginger extract\",\n",
    "    \"garlic\", \"garlic extract\", \"garlic powder\",\n",
    "    \"cinnamon\", \"cinnamon extract\",\n",
    "    \"black seed\", \"black seed oil\", \"nigella sativa\",\n",
    "    \"evening primrose\", \"evening primrose oil\",\n",
    "    \"flaxseed\", \"flaxseed oil\", \"fish oil\",\n",
    "    \"peppermint\", \"peppermint oil\",\n",
    "    \"chamomile\", \"chamomile extract\",\n",
    "    \"echinacea\", \"valerian\", \"ashwagandha\",\n",
    "    \"msm\", \"methylsulfonylmethane\",\n",
    "    \"lutein\", \"zeaxanthin\", \"lycopene\", \"astaxanthin\",\n",
    "    \"resveratrol\", \"quercetin\",\n",
    "    \"spirulina\", \"chlorella\",\n",
    "    \"milk thistle\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# R4 — Spell-fix Dictionary (ONLY true typos — NO synonym merging per P0.1)\n",
    "# Keys are sorted longest-first so longer matches win over shorter substrings.\n",
    "# =============================================================================\n",
    "SPELL_FIX = {\n",
    "    \"benzylpenicillin sodiium\":  \"benzylpenicillin sodium\",\n",
    "    \"cholorohexidine\":           \"chlorhexidine\",\n",
    "    \"chlorohexidine\":            \"chlorhexidine\",\n",
    "    \"chlorohexidin\":             \"chlorhexidine\",\n",
    "    \"nitrofurantion\":            \"nitrofurantoin\",\n",
    "    \"immunoglobulins\":           \"immunoglobulin\",\n",
    "    \"panthenoll\":                \"panthenol\",\n",
    "    \"pantheno\":                  \"panthenol\",\n",
    "    \"pilocarpin\":                \"pilocarpine\",\n",
    "    \"macrophages\":               \"macrophage\",\n",
    "    \"macrofage\":                 \"macrophage\",\n",
    "    \"olanzapin\":                 \"olanzapine\",\n",
    "    \"diclofienac\":               \"diclofenac\",\n",
    "    \"sildeanfil\":                \"sildenafil\",\n",
    "    \"sindalfil\":                 \"sildenafil\",\n",
    "    \"digoxine\":                  \"digoxin\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Manual Replacements — Applied BEFORE generic split\n",
    "# =============================================================================\n",
    "PLAIN_REPLACEMENTS = {\n",
    "    \"vit.\": \"vitamin \",\n",
    "    \"vitamin b complex\": (\n",
    "        \"thiamine + riboflavin + niacin + pantothenic acid + \"\n",
    "        \"pyridoxine + biotin + folic acid + cobalamin\"\n",
    "    ),\n",
    "    \"b complex\": (\n",
    "        \"thiamine + riboflavin + niacin + pantothenic acid + \"\n",
    "        \"pyridoxine + biotin + folic acid + cobalamin\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "BVITAMIN_REGEX = [\n",
    "    (re.compile(r'\\bvit\\b\\.?'),  \"vitamin \"),\n",
    "    (re.compile(r'\\bb12\\b'),     \"cobalamin\"),\n",
    "    (re.compile(r'\\bb9\\b'),      \"folic acid\"),\n",
    "    (re.compile(r'\\bb7\\b'),      \"biotin\"),\n",
    "    (re.compile(r'\\bb6\\b'),      \"pyridoxine\"),\n",
    "    (re.compile(r'\\bb5\\b'),      \"pantothenic acid\"),\n",
    "    (re.compile(r'\\bb3\\b'),      \"niacin\"),\n",
    "    (re.compile(r'\\bb2\\b'),      \"riboflavin\"),\n",
    "    (re.compile(r'\\bb1\\b'),      \"thiamine\"),\n",
    "]\n",
    "\n",
    "REPLACEMENTS = PLAIN_REPLACEMENTS\n",
    "\n",
    "# =============================================================================\n",
    "# Known Ingredient Vocabulary — Used for garbage phrase & unknown token detection\n",
    "# =============================================================================\n",
    "KNOWN_INGREDIENT_KEYWORDS = {\n",
    "    \"vitamin\", \"acid\", \"calcium\", \"magnesium\", \"zinc\", \"iron\", \"sodium\",\n",
    "    \"potassium\", \"chloride\", \"oxide\", \"hydrochloride\", \"sulfate\", \"phosphate\",\n",
    "    \"gluconate\", \"citrate\", \"acetate\", \"lactate\", \"carbonate\", \"nitrate\",\n",
    "    \"immunoglobulin\", \"albumin\", \"insulin\", \"heparin\", \"factor\", \"hormone\",\n",
    "    \"enzyme\", \"extract\", \"compound\", \"complex\", \"analog\", \"analogue\",\n",
    "    \"colony\", \"stimulating\", \"granulocyte\", \"macrophage\", \"ketoanalogue\",\n",
    "    \"histidine\", \"lysine\", \"threonine\", \"tryptophan\", \"tyrosine\", \"amino\",\n",
    "    \"iodo\", \"chloro\", \"hydroxy\", \"quinoline\", \"biotin\", \"niacin\", \"riboflavin\",\n",
    "    \"pantothenic\", \"pyridoxine\", \"thiamine\", \"cobalamin\", \"folic\", \"selenium\",\n",
    "    \"manganese\", \"copper\", \"boron\", \"chromium\", \"molybdenum\", \"fluoride\",\n",
    "    \"iodochlorohydroxyquinoline\", \"panthenol\", \"pilocarpine\", \"omega\",\n",
    "    \"retinol\", \"tocopherol\", \"ascorbic\", \"cholecalciferol\", \"ergocalciferol\",\n",
    "    \"menadione\", \"phytomenadione\", \"alpha\", \"beta\", \"gamma\", \"delta\",\n",
    "    \"methionine\", \"cysteine\", \"arginine\", \"leucine\", \"isoleucine\", \"valine\",\n",
    "    \"alanine\", \"glycine\", \"proline\", \"serine\", \"glutamine\", \"asparagine\",\n",
    "    \"aspartate\", \"glutamate\", \"phenylalanine\",\n",
    "    \"coenzyme\", \"ubiquinone\", \"carnitine\", \"taurine\", \"inositol\", \"choline\",\n",
    "    \"lipoic\", \"rutin\", \"hesperidin\", \"quercetin\", \"flavonoid\",\n",
    "    \"glucosamine\", \"chondroitin\", \"collagen\", \"hyaluronic\",\n",
    "    \"probiotic\", \"prebiotic\", \"lactobacillus\", \"bifidobacterium\",\n",
    "    \"interferon\", \"erythropoietin\", \"filgrastim\", \"pegfilgrastim\",\n",
    "    \"antitoxin\", \"antivenom\", \"vaccine\",\n",
    "    \"paracetamol\", \"acetaminophen\", \"ibuprofen\", \"aspirin\", \"caffeine\",\n",
    "    \"codeine\", \"morphine\", \"tramadol\", \"diclofenac\", \"naproxen\",\n",
    "    \"amoxicillin\", \"ampicillin\", \"penicillin\", \"cephalexin\", \"azithromycin\",\n",
    "    \"ciprofloxacin\", \"metronidazole\", \"doxycycline\", \"tetracycline\",\n",
    "    \"metformin\", \"glibenclamide\", \"atorvastatin\", \"simvastatin\",\n",
    "    \"amlodipine\", \"enalapril\", \"losartan\", \"hydrochlorothiazide\", \"furosemide\",\n",
    "    \"omeprazole\", \"ranitidine\", \"metoclopramide\", \"domperidone\", \"ondansetron\",\n",
    "    \"salbutamol\", \"terbutaline\", \"beclomethasone\", \"fluticasone\", \"ipratropium\",\n",
    "    \"prednisolone\", \"dexamethasone\", \"hydrocortisone\", \"betamethasone\",\n",
    "    \"loratadine\", \"cetirizine\", \"diphenhydramine\", \"promethazine\",\n",
    "    \"diazepam\", \"alprazolam\", \"lorazepam\", \"clonazepam\", \"phenobarbital\",\n",
    "    \"haloperidol\", \"risperidone\", \"olanzapine\", \"quetiapine\", \"aripiprazole\",\n",
    "    \"fluoxetine\", \"sertraline\", \"paroxetine\", \"escitalopram\", \"venlafaxine\",\n",
    "    \"levothyroxine\", \"propylthiouracil\", \"methimazole\",\n",
    "    \"warfarin\", \"enoxaparin\", \"clopidogrel\",\n",
    "    \"cyclosporine\", \"tacrolimus\", \"mycophenolate\", \"azathioprine\",\n",
    "    \"methotrexate\", \"cyclophosphamide\", \"doxorubicin\", \"fluorouracil\",\n",
    "    \"sildenafil\", \"tadalafil\", \"testosterone\", \"estradiol\", \"progesterone\",\n",
    "    \"spironolactone\", \"dandelion\", \"silymarin\", \"iodine\",\n",
    "    \"poliomyelitis\", \"inactivated\", \"attenuated\", \"poliovirus\",\n",
    "    \"willebrand\", \"von\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# R5 — Cosmetic / Personal-Care Terms\n",
    "# Row is DELETED if 2+ of these terms are found in the entry.\n",
    "# =============================================================================\n",
    "COSMETIC_TERMS = {\n",
    "    \"cream\", \"shampoo\", \"lotion\", \"styling\", \"smooth\", \"hair\", \"gel\",\n",
    "    \"serum\", \"moisturizer\", \"conditioner\", \"spray\", \"foam\", \"mask\",\n",
    "    \"scrub\", \"toner\", \"cleanser\", \"balm\", \"wax\", \"polish\",\n",
    "    \"blush\", \"foundation\", \"lipstick\", \"mascara\", \"perfume\",\n",
    "    \"fragrance\", \"deodorant\", \"sunscreen\", \"exfoliant\", \"primer\",\n",
    "    \"scalp\", \"skin\", \"whitening\", \"regen\", \"matrix\", \"photostable\",\n",
    "    \"uva\", \"uvb\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# R6 — Vague Category Terms (Keep row + flag = VAGUE_CATEGORY)\n",
    "# =============================================================================\n",
    "VAGUE_CATEGORY_EXACT = {\n",
    "    \"minerals\", \"elements\", \"omega\", \"ors\", \"carbohydrates\",\n",
    "    \"proteins\", \"multivitamin\", \"multivitamins\",\n",
    "    \"vitamins and minerals\", \"vitamins\", \"trace elements\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# R7 — Truncated Token Detection (Keep row + flag = TRUNCATED)\n",
    "# =============================================================================\n",
    "TRUNCATED_TOKENS = {\n",
    "    \"ethinyl\", \"mono\", \"hydro\", \"peg\", \"poly\",\n",
    "    \"micronized alpha\", \"micronized\", \"dehydro\", \"desoxy\", \"nor\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# R8 — Unknown Token Detection\n",
    "# =============================================================================\n",
    "SHORT_VALID_TOKENS = {\n",
    "    \"a\", \"c\", \"d\", \"e\", \"k\",\n",
    "    \"d2\", \"d3\", \"k1\", \"k2\", \"k3\",\n",
    "    \"b1\", \"b2\", \"b3\", \"b5\", \"b6\", \"b7\", \"b9\", \"b12\",\n",
    "    \"ors\", \"rna\", \"dna\", \"hiv\", \"ige\", \"igg\", \"iga\", \"igm\",\n",
    "    \"atp\", \"adp\", \"nad\", \"gmp\", \"amp\",\n",
    "    \"viii\", \"vii\", \"vi\", \"iv\", \"xii\", \"xiii\",\n",
    "}\n",
    "\n",
    "UNKNOWN_TOKEN_PATTERN = re.compile(\n",
    "    r'^[a-z]{1,3}\\d+$'\n",
    "    r'|^[a-z0-9]{1,3}\\s[a-z0-9]{1,2}$'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Safe vitamin letters/codes — single-letter tokens that are valid vitamins\n",
    "# =============================================================================\n",
    "VALID_VITAMIN_LETTERS = {\n",
    "    \"a\", \"c\", \"d\", \"e\", \"k\",\n",
    "    \"d2\", \"d3\", \"k1\", \"k2\", \"k3\",\n",
    "    \"b1\", \"b2\", \"b3\", \"b5\", \"b6\", \"b7\", \"b9\", \"b12\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Pattern for inserting '+' between unseparated known ingredients\n",
    "# =============================================================================\n",
    "_UNSEP = (\n",
    "    r'calcium|magnesium|zinc|iron|selenium|manganese|copper|boron|chromium|'\n",
    "    r'molybdenum|fluoride|biotin|niacin|riboflavin|thiamine|pyridoxine|'\n",
    "    r'pantothenic|folic|cobalamin|lysine|histidine|threonine|tryptophan|'\n",
    "    r'tyrosine|iodine|iodo|granulocyte|albumin|insulin|heparin|collagen|'\n",
    "    r'glucosamine|chondroitin|carnitine|taurine|inositol|choline|'\n",
    "    r'ubiquinone|rutin|hesperidin|quercetin|lipoic|coenzyme|'\n",
    "    r'lactobacillus|bifidobacterium|probiotic|prebiotic|'\n",
    "    r'paracetamol|acetaminophen|ibuprofen|aspirin|caffeine|codeine|'\n",
    "    r'amoxicillin|ampicillin|penicillin|ciprofloxacin|metronidazole|'\n",
    "    r'metformin|atorvastatin|simvastatin|amlodipine|enalapril|losartan|'\n",
    "    r'omeprazole|ranitidine|ondansetron|salbutamol|terbutaline|'\n",
    "    r'prednisolone|dexamethasone|hydrocortisone|betamethasone|loratadine|'\n",
    "    r'cetirizine|diazepam|alprazolam|fluoxetine|sertraline|levothyroxine|'\n",
    "    r'warfarin|cyclosporine|methotrexate|sildenafil|testosterone|estradiol|'\n",
    "    r'progesterone|spironolactone|dandelion|silymarin|retinol|tocopherol|'\n",
    "    r'ascorbic|cholecalciferol|menadione'\n",
    ")\n",
    "UNSEPARATED_SPLIT_PATTERN = re.compile(\n",
    "    r'(?<=[a-z\\d])\\s+(?=(' + _UNSEP + r')\\b)'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# R3 — Dosage unit pattern\n",
    "# =============================================================================\n",
    "DOSE_UNIT_PATTERN = re.compile(\n",
    "    r'\\d+(\\.\\d+)?\\s*(mg|g|gm|mcg|µg|ug|iu|i\\s*u|miu|ml|%|units?|tabs?|caps?|amp|vial)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "LEADING_NUMBER_PATTERN = re.compile(\n",
    "    r'^\\s*[\\d\\s\\.]+\\s*'\n",
    "    r'(mg|g|gm|mcg|µg|ug|iu|i\\s*u|miu|ml|%|units?|tabs?|caps?|amp|vial)?\\s*$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Logging\n",
    "# =============================================================================\n",
    "def log_message(message):\n",
    "    \"\"\"Log messages to console and file with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    msg = f\"[{timestamp}] {message}\"\n",
    "    print(msg)\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# =============================================================================\n",
    "# Step 0 — Encoded Token Decoder\n",
    "# =============================================================================\n",
    "def decode_encoded_tokens(text):\n",
    "    \"\"\"Replace __INGxxxx__ placeholders with known decoded values.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for token, replacement in ENCODED_TOKEN_MAP.items():\n",
    "        text = text.replace(token, replacement)\n",
    "    text = re.sub(r'__[A-Z]+\\d+__', ' ', text)\n",
    "    return text\n",
    "\n",
    "# =============================================================================\n",
    "# R4 — Spell Correction\n",
    "# =============================================================================\n",
    "def apply_spell_fix(text):\n",
    "    \"\"\"Apply spelling corrections using word boundaries. No synonym merging.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for wrong, right in sorted(SPELL_FIX.items(), key=lambda x: -len(x[0])):\n",
    "        if wrong in text:\n",
    "            text = re.sub(r'\\b' + re.escape(wrong) + r'\\b', right, text)\n",
    "    return text\n",
    "\n",
    "# =============================================================================\n",
    "# Garbage Detection\n",
    "# =============================================================================\n",
    "def is_garbage_token(token):\n",
    "    \"\"\"Return True if an ingredient token should be discarded.\"\"\"\n",
    "    t = token.strip().lower()\n",
    "    if not t or len(t) <= 2:\n",
    "        return True\n",
    "    if t in GARBAGE_EXACT:\n",
    "        return True\n",
    "    if t in GARBAGE_TOKENS_EXACT:\n",
    "        return True\n",
    "    if t in NON_DRUG_TOKENS:\n",
    "        return True\n",
    "    for g in GARBAGE_EXACT:\n",
    "        if len(g) >= 8 and g in t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_likely_garbage_phrase(text):\n",
    "    \"\"\"Detect multi-word free-text with no recognizable ingredient vocabulary.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) < 3:\n",
    "        return False\n",
    "    matches = sum(\n",
    "        1 for w in words\n",
    "        if any(kw in w for kw in KNOWN_INGREDIENT_KEYWORDS)\n",
    "    )\n",
    "    return matches == 0\n",
    "\n",
    "# =============================================================================\n",
    "# R5 — Cosmetic Entry Detection\n",
    "# =============================================================================\n",
    "def is_cosmetic_entry(text):\n",
    "    \"\"\"Return True if the entry looks like a cosmetic/personal-care product.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    words = set(text.lower().split())\n",
    "    return len(words & COSMETIC_TERMS) >= 2\n",
    "\n",
    "# =============================================================================\n",
    "# R6 — Vague Category Detection\n",
    "# =============================================================================\n",
    "def is_vague_category(text):\n",
    "    \"\"\"Return True if the entire entry is a vague/non-specific category.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    t = text.strip().lower()\n",
    "    if t in VAGUE_CATEGORY_EXACT:\n",
    "        return True\n",
    "    tokens = [p.strip() for p in t.split('+')]\n",
    "    return all(tok in VAGUE_CATEGORY_EXACT for tok in tokens if tok)\n",
    "\n",
    "# =============================================================================\n",
    "# R7 — Truncated Token Detection\n",
    "# =============================================================================\n",
    "def has_truncated_token(parts):\n",
    "    \"\"\"Return True if any part looks like a truncated/incomplete ingredient.\"\"\"\n",
    "    for p in parts:\n",
    "        p_lower = p.strip().lower()\n",
    "        if p_lower in TRUNCATED_TOKENS:\n",
    "            return True\n",
    "        if re.match(r'^(hydro|mono|poly|peg|dehydro|desoxy|nor)$', p_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# =============================================================================\n",
    "# R8 — Unknown Token Classification\n",
    "# =============================================================================\n",
    "def classify_token(token):\n",
    "    \"\"\"Classify token as 'valid' or 'unknown'.\"\"\"\n",
    "    t = token.strip().lower()\n",
    "    if not t:\n",
    "        return 'valid'\n",
    "    if t in SHORT_VALID_TOKENS:\n",
    "        return 'valid'\n",
    "    if t.startswith('vitamin '):\n",
    "        return 'valid'\n",
    "    for kw in KNOWN_INGREDIENT_KEYWORDS:\n",
    "        if kw in t:\n",
    "            return 'valid'\n",
    "    if t in SPELL_FIX.values():\n",
    "        return 'valid'\n",
    "    if UNKNOWN_TOKEN_PATTERN.match(t):\n",
    "        return 'unknown'\n",
    "    if len(t) <= 3 and t not in SHORT_VALID_TOKENS:\n",
    "        return 'unknown'\n",
    "    return 'valid'\n",
    "\n",
    "# =============================================================================\n",
    "# R11 — Omega Format Normalization\n",
    "# =============================================================================\n",
    "def normalize_omega(text):\n",
    "    \"\"\"Normalize omega patterns into consistent tokens.\"\"\"\n",
    "    def expand_omega_multi(m):\n",
    "        nums = m.group(1).split('-')\n",
    "        return ' + '.join('omega ' + n for n in nums)\n",
    "\n",
    "    text = re.sub(r'\\bomega-(\\d+(?:-\\d+)+)\\b', expand_omega_multi, text)\n",
    "    text = re.sub(r'\\bomega-(\\d+)\\b', r'omega \\1', text)\n",
    "\n",
    "    def expand_omega_spaced(m):\n",
    "        nums = m.group(1).strip().split()\n",
    "        if len(nums) > 1:\n",
    "            return ' + '.join('omega ' + n for n in nums)\n",
    "        return 'omega ' + nums[0]\n",
    "\n",
    "    text = re.sub(r'\\bomega\\s+(\\d+(?:\\s+\\d+)+)\\b', expand_omega_spaced, text)\n",
    "    return text\n",
    "\n",
    "# =============================================================================\n",
    "# Text Normalization\n",
    "# =============================================================================\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize raw text into a consistent ' + ' separated format.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = text.strip()\n",
    "    if not text or text.lower() in GARBAGE_EXACT:\n",
    "        return None\n",
    "\n",
    "    text = text.lower()\n",
    "    text = apply_spell_fix(text)\n",
    "\n",
    "    for wrong, right in PLAIN_REPLACEMENTS.items():\n",
    "        if wrong in text:\n",
    "            text = text.replace(wrong, right)\n",
    "\n",
    "    for pattern, right in BVITAMIN_REGEX:\n",
    "        text = pattern.sub(right, text)\n",
    "\n",
    "    text = normalize_omega(text)\n",
    "    text = re.sub(r'\\bvit\\.?\\s+', 'vitamin ', text)\n",
    "\n",
    "    text = re.sub(r'\\bvitamin\\s+amins?\\b', 'vitamin', text)\n",
    "    text = re.sub(r'\\bamins?\\b', '', text)\n",
    "\n",
    "    text = re.sub(r'\\(([^)]*)\\)', r' + \\1', text)\n",
    "    text = re.sub(r'(?<!\\+)\\s+(vitamin\\s)', r' + \\1', text)\n",
    "    text = UNSEPARATED_SPLIT_PATTERN.sub(' + ', text)\n",
    "    text = re.sub(r'(--|–|-|/|,|;|\\|&| and | with |&|\\+)', ' + ', text)\n",
    "\n",
    "    _TYPE_MEDICAL_CONTEXT = {\n",
    "        \"poliomyelitis\", \"poliovirus\", \"vaccine\", \"hepatitis\",\n",
    "        \"diphtheria\", \"pertussis\", \"meningitis\", \"rotavirus\",\n",
    "        \"herpes\", \"adenovirus\", \"coronavirus\", \"influenza\",\n",
    "        \"dengue\", \"rabies\", \"typhoid\", \"cholera\",\n",
    "        \"collagen\", \"diabetes\",\n",
    "    }\n",
    "    _full_has_type_context = any(kw in text for kw in _TYPE_MEDICAL_CONTEXT)\n",
    "    _type_protected = {}\n",
    "\n",
    "    def _protect_type_smart(m):\n",
    "        if _full_has_type_context:\n",
    "            key = \"__TYPEPROT\" + str(len(_type_protected)) + \"__\"\n",
    "            _type_protected[key] = m.group(0).strip()\n",
    "            return key\n",
    "        return \" \"\n",
    "\n",
    "    text = re.sub(r'\\btype\\s+\\d+\\b', _protect_type_smart, text)\n",
    "\n",
    "    text = DOSE_UNIT_PATTERN.sub(' ', text)\n",
    "    text = re.sub(r'\\b\\d{2,}\\b', ' ', text)\n",
    "    text = re.sub(r'(?<=[a-z])\\s+\\d+(?:\\s+\\d+)*\\s*$', ' ', text)\n",
    "    text = re.sub(r'(?<=[a-z])\\s+0\\s+\\d+', ' ', text)\n",
    "    text = re.sub(r'(?<=[a-z])\\s+\\d\\s+\\d+\\b', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\bomega\\s+(\\d)\\b', r'omega__OMGPROT__\\1', text)\n",
    "    text = re.sub(r'\\btype\\s+(\\d)\\b', r'type__TYPROT__\\1', text)\n",
    "    text = re.sub(r'(?<=[a-z])\\s+\\d\\b', ' ', text)\n",
    "    text = text.replace('omega__OMGPROT__', 'omega ')\n",
    "    text = text.replace('type__TYPROT__', 'type ')\n",
    "\n",
    "    for key, val in _type_protected.items():\n",
    "        text = text.replace(key, val)\n",
    "\n",
    "    text = re.sub(r'[^a-z0-9+\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s*\\+\\s*', ' + ', text).strip()\n",
    "    text = text.strip('+ ')\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "# =============================================================================\n",
    "# Vitamin Shortcut Expansion\n",
    "# =============================================================================\n",
    "def expand_vitamin_shortcuts(parts, original_text):\n",
    "    \"\"\"Expand vitamin letter tokens when the original text includes 'vitamin'.\"\"\"\n",
    "    out = []\n",
    "    has_vitamin = \"vitamin\" in (original_text or \"\")\n",
    "\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if not p:\n",
    "            continue\n",
    "        if p.startswith(\"vitamin \"):\n",
    "            out.append(p)\n",
    "            continue\n",
    "        if has_vitamin and p in VALID_VITAMIN_LETTERS:\n",
    "            out.append(f\"vitamin {p}\")\n",
    "        else:\n",
    "            out.append(p)\n",
    "\n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# R3.1 — Remove Leading Numeric Tokens\n",
    "# =============================================================================\n",
    "def remove_leading_numeric_tokens(parts):\n",
    "    \"\"\"Drop tokens from the start that are purely numeric or dose-only.\"\"\"\n",
    "    while parts:\n",
    "        first = parts[0].strip()\n",
    "        if LEADING_NUMBER_PATTERN.match(first):\n",
    "            parts = parts[1:]\n",
    "        elif re.match(r'^\\d+$', first):\n",
    "            parts = parts[1:]\n",
    "        else:\n",
    "            break\n",
    "    return parts\n",
    "\n",
    "# =============================================================================\n",
    "# Clean Individual Ingredients List\n",
    "# =============================================================================\n",
    "def clean_ingredient_list(parts, original_text):\n",
    "    \"\"\"Clean and validate ingredient tokens; return cleaned tokens and flags.\"\"\"\n",
    "    parts = expand_vitamin_shortcuts(parts, original_text)\n",
    "    parts = remove_leading_numeric_tokens(parts)\n",
    "\n",
    "    cleaned = []\n",
    "    token_flags = []\n",
    "\n",
    "    for ingredient in parts:\n",
    "        ingredient = re.sub(r'\\s+', ' ', ingredient).strip()\n",
    "        ingredient = re.sub(r'\\bamin[s]?\\b', '', ingredient).strip()\n",
    "        ingredient = re.sub(r'\\b([a-z]{4,})\\d+$', r'\\1', ingredient)\n",
    "        ingredient = re.sub(r'\\s+', ' ', ingredient).strip()\n",
    "\n",
    "        if is_garbage_token(ingredient):\n",
    "            continue\n",
    "        if len(ingredient) <= 2:\n",
    "            continue\n",
    "\n",
    "        token_class = classify_token(ingredient)\n",
    "        if token_class == 'unknown':\n",
    "            token_flags.append((ingredient, 'UNKNOWN'))\n",
    "\n",
    "        cleaned.append(ingredient)\n",
    "\n",
    "    return sorted(set(cleaned)), token_flags\n",
    "\n",
    "# =============================================================================\n",
    "# Main Per-Row Cleaning Function\n",
    "# =============================================================================\n",
    "def clean_active_ingredient(text):\n",
    "    \"\"\"Clean a single row of active-ingredient text and return result + flags.\"\"\"\n",
    "    row_flags = []\n",
    "    unknown_tokens = []\n",
    "\n",
    "    text = decode_encoded_tokens(text)\n",
    "\n",
    "    normalized = normalize_text(text)\n",
    "    if normalized is None:\n",
    "        return {\"result\": None, \"row_flag\": \"\", \"unknown_tokens\": []}\n",
    "\n",
    "    if is_likely_garbage_phrase(normalized):\n",
    "        return {\"result\": None, \"row_flag\": \"\", \"unknown_tokens\": []}\n",
    "\n",
    "    if is_cosmetic_entry(normalized):\n",
    "        return {\"result\": None, \"row_flag\": \"COSMETIC\", \"unknown_tokens\": []}\n",
    "\n",
    "    parts = [p.strip() for p in normalized.split('+')]\n",
    "    cleaned_parts, token_flags = clean_ingredient_list(parts, normalized)\n",
    "\n",
    "    if not cleaned_parts:\n",
    "        return {\"result\": None, \"row_flag\": \"\", \"unknown_tokens\": []}\n",
    "\n",
    "    joined = \" + \".join(cleaned_parts)\n",
    "\n",
    "    if is_vague_category(joined):\n",
    "        row_flags.append(\"VAGUE_CATEGORY\")\n",
    "\n",
    "    if has_truncated_token(cleaned_parts):\n",
    "        row_flags.append(\"TRUNCATED\")\n",
    "\n",
    "    if token_flags:\n",
    "        unknown_tokens = [t for t, _ in token_flags]\n",
    "        row_flags.append(\"HAS_UNKNOWN_TOKENS\")\n",
    "\n",
    "    if cleaned_parts and re.match(r'^\\d+', cleaned_parts[0]):\n",
    "        row_flags.append(\"LEADING_NUMBER_UNRESOLVED\")\n",
    "\n",
    "    return {\n",
    "        \"result\": joined,\n",
    "        \"row_flag\": \",\".join(row_flags) if row_flags else \"\",\n",
    "        \"unknown_tokens\": unknown_tokens,\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline\n",
    "# =============================================================================\n",
    "def clean_drug_ingredients(input_path, output_path):\n",
    "    \"\"\"Full cleaning pipeline with logging and statistics.\"\"\"\n",
    "    log_message(\"=\" * 70)\n",
    "    log_message(\"DRUG INGREDIENT CLEANING PIPELINE v2 - STARTED\")\n",
    "    log_message(\"=\" * 70)\n",
    "\n",
    "    log_message(f\"Loading data from: {input_path}\")\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "    initial_count = len(df)\n",
    "    log_message(f\"Loaded {initial_count:,} rows\")\n",
    "    log_message(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "    df.columns = df.columns.str.strip()\n",
    "    possible_cols = [\n",
    "        'ActiveIngredient', 'activeingredient', 'active_ingredient',\n",
    "        'Generic Name', 'generic name', 'GenericName',\n",
    "        'Ingredients', 'ingredients',\n",
    "    ]\n",
    "\n",
    "    col_name = None\n",
    "    for col in possible_cols:\n",
    "        if col in df.columns:\n",
    "            col_name = col\n",
    "            break\n",
    "\n",
    "    if col_name is None:\n",
    "        col_lower_map = {c.lower(): c for c in df.columns}\n",
    "        for col in possible_cols:\n",
    "            if col.lower() in col_lower_map:\n",
    "                col_name = col_lower_map[col.lower()]\n",
    "                break\n",
    "\n",
    "    if col_name is None:\n",
    "        raise ValueError(\n",
    "            \"Active ingredient column not found. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    log_message(f\"Found ingredient column: '{col_name}'\")\n",
    "\n",
    "    encoded_mask = df[col_name].str.contains(r'__[A-Z]+\\d+__', na=False, regex=True)\n",
    "    encoded_count = encoded_mask.sum()\n",
    "    if encoded_count > 0:\n",
    "        log_message(f\"Found {encoded_count:,} rows with encoded tokens — decoding...\")\n",
    "        top_tokens = (\n",
    "            df[encoded_mask][col_name]\n",
    "            .str.findall(r'__[A-Z]+\\d+__')\n",
    "            .explode()\n",
    "            .value_counts()\n",
    "            .head(20)\n",
    "        )\n",
    "        log_message(\"Top encoded tokens:\\n\" + top_tokens.to_string())\n",
    "    else:\n",
    "        log_message(\"No encoded tokens detected.\")\n",
    "\n",
    "    log_message(\"Starting cleaning process...\")\n",
    "    cleaning_results = df[col_name].apply(clean_active_ingredient)\n",
    "\n",
    "    df['activeingredient_clean'] = cleaning_results.apply(lambda x: x['result'])\n",
    "    df['row_flag'] = cleaning_results.apply(lambda x: x['row_flag'])\n",
    "    df['unknown_tokens'] = cleaning_results.apply(\n",
    "        lambda x: \"|\".join(x['unknown_tokens']) if x['unknown_tokens'] else \"\"\n",
    "    )\n",
    "\n",
    "    df['Graph_Node_Ingredient'] = df['activeingredient_clean']\n",
    "\n",
    "    df['ingredient_count'] = df['Graph_Node_Ingredient'].apply(\n",
    "        lambda x: len(x.split(' + ')) if pd.notna(x) else 0\n",
    "    )\n",
    "    df['is_combination'] = df['ingredient_count'] > 1\n",
    "    df['combo_type'] = df['ingredient_count'].apply(\n",
    "        lambda n: 'single' if n == 1 else ('combo' if n > 1 else np.nan)\n",
    "    )\n",
    "\n",
    "    df_valid = df[\n",
    "        df['Graph_Node_Ingredient'].notna() &\n",
    "        (df['ingredient_count'] > 0) &\n",
    "        (df['row_flag'] == \"\")\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    removed_count = initial_count - len(df_valid)\n",
    "\n",
    "    cols_to_drop = [c for c in ['row_flag', 'unknown_tokens'] if c in df_valid.columns]\n",
    "    df_final = df_valid.drop(columns=cols_to_drop)\n",
    "    df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "    log_message(\"\")\n",
    "    log_message(\"=\" * 70)\n",
    "    log_message(\"CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "    log_message(\"=\" * 70)\n",
    "    log_message(f\"Total rows processed:     {initial_count:,}\")\n",
    "    log_message(f\"Valid rows retained:      {len(df_valid):,}\")\n",
    "    log_message(f\"Invalid rows removed:     {removed_count:,}  \"\n",
    "                f\"({removed_count / initial_count * 100:.1f}%)\")\n",
    "    log_message(f\"Single ingredients:       {(df_valid['combo_type'] == 'single').sum():,}\")\n",
    "    log_message(f\"Combination drugs:        {(df_valid['combo_type'] == 'combo').sum():,}\")\n",
    "    log_message(f\"Average ingredients:      {df_valid['ingredient_count'].mean():.2f}\")\n",
    "    log_message(f\"Max ingredients in combo: {df_valid['ingredient_count'].max()}\")\n",
    "\n",
    "    flag_series = df_valid['row_flag'].str.split(',', expand=True).stack()\n",
    "    flag_counts = flag_series[flag_series != ''].value_counts()\n",
    "    if not flag_counts.empty:\n",
    "        log_message(\"\\nRow flag summary (all should be empty — shown for debug):\")\n",
    "        for flag, cnt in flag_counts.items():\n",
    "            log_message(f\"  {flag:<35s}: {cnt:,}\")\n",
    "    else:\n",
    "        log_message(\"\\nNo flagged rows in output (all clean).\")\n",
    "\n",
    "    log_message(\"\")\n",
    "    log_message(f\"Output saved to: {output_path}\")\n",
    "    log_message(\"=\" * 70)\n",
    "\n",
    "    return df_valid\n",
    "\n",
    "# =============================================================================\n",
    "# Sanity-Check Helper\n",
    "# =============================================================================\n",
    "def test_samples():\n",
    "    \"\"\"Run cleaning on known edge-case samples and print results.\"\"\"\n",
    "    samples = [\n",
    "        (\"selected theraputically active gereinigter honig\", None),\n",
    "        (\"biotin + folic acid + iron vitamin c folic acid vitamin thiamine + niacin + pantothenic acid + pyridoxine + riboflavin\",\n",
    "         \"biotin + folic acid + iron + niacin + pantothenic acid + pyridoxine + riboflavin + thiamine + vitamin c\"),\n",
    "        (\"human normal immunoglobulins\", \"human normal immunoglobulin\"),\n",
    "        (\"iodochlorohydroxyquinoline\", \"iodochlorohydroxyquinoline\"),\n",
    "        (\"dr ey t\", None),\n",
    "        (\"calcium vitamin d3 vitamin k2 zinc boron copper manganese selenium magnesium\",\n",
    "         \"boron + calcium + copper + magnesium + manganese + selenium + vitamin d3 + vitamin k2 + zinc\"),\n",
    "        (\"alpha ketoanalogue of amino acids + histidine + lysine + threonine + tryptophan + tyrosine\",\n",
    "         \"alpha ketoanalogue of amino acids + histidine + lysine + threonine + tryptophan + tyrosine\"),\n",
    "        (\"granulocyte macrofage colony stimulating factor\",\n",
    "         \"granulocyte macrophage colony stimulating factor\"),\n",
    "        (\"__ING0035__2 + dandelion + folic acid + selenium + silymarin + vitamin c + vitamin e + zinc\",\n",
    "         \"dandelion + folic acid + selenium + silymarin + vitamin c + vitamin e + zinc\"),\n",
    "        (\"__ING0024__mins + __ING0035__2 + copper + folic acid + iodine + iron + niacin + pyridoxine + riboflavin + selenium + thiamine + zinc\",\n",
    "         \"copper + folic acid + iodine + iron + niacin + pyridoxine + riboflavin + selenium + thiamine + zinc\"),\n",
    "        (\"350m + cream + hair + smooth + styling\", None),\n",
    "        (\"sp__ING0055__olactone\", \"spironolactone\"),\n",
    "        (\"__ING0024__mins\", None),\n",
    "        (\"omega-3 + vitamin e\", \"omega 3 + vitamin e\"),\n",
    "        (\"omega-3-6-9 + vitamin c\", \"omega 3 + omega 6 + omega 9 + vitamin c\"),\n",
    "        (\"150 + alpha + folic acid + iron\", \"alpha + folic acid + iron\"),\n",
    "        (\"1000 + folic acid + vitamin b12\", \"cobalamin + folic acid\"),\n",
    "        (\"cholorohexidine\", \"chlorhexidine\"),\n",
    "        (\"digoxine\", \"digoxin\"),\n",
    "        (\"panthenoll\", \"panthenol\"),\n",
    "        (\"paracetamol\", \"paracetamol\"),\n",
    "        (\"acetaminophen\", \"acetaminophen\"),\n",
    "        (\"cream + hair + smooth + styling\", None),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SANITY CHECK — EDGE CASE SAMPLES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    passed = failed = 0\n",
    "\n",
    "    for raw, expected in samples:\n",
    "        res = clean_active_ingredient(raw)\n",
    "        result = res['result']\n",
    "        flag = res['row_flag']\n",
    "\n",
    "        ok = (result is None and expected is None) or (result == expected)\n",
    "        icon = \"PASS\" if ok else \"FAIL\"\n",
    "        passed += ok\n",
    "        failed += (not ok)\n",
    "\n",
    "        print(f\"\\n[{icon}]\")\n",
    "        print(f\"  INPUT   : {raw}\")\n",
    "        print(f\"  OUTPUT  : {result}\")\n",
    "        print(f\"  FLAGS   : {flag or '(none)'}\")\n",
    "        if not ok:\n",
    "            print(f\"  EXPECTED: {expected}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Results: {passed} passed, {failed} failed out of {len(samples)} tests\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    return failed == 0\n",
    "\n",
    "# =============================================================================\n",
    "# Encoded Token Audit Helper\n",
    "# =============================================================================\n",
    "def audit_encoded_tokens(input_path):\n",
    "    \"\"\"Scan the dataset and print unique encoded tokens with example rows.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    possible_cols = [\n",
    "        'ActiveIngredient', 'activeingredient', 'active_ingredient',\n",
    "        'Generic Name', 'generic name', 'GenericName',\n",
    "        'Ingredients', 'ingredients',\n",
    "    ]\n",
    "    col_name = None\n",
    "    for col in possible_cols:\n",
    "        if col in df.columns:\n",
    "            col_name = col\n",
    "            break\n",
    "    if col_name is None:\n",
    "        col_lower_map = {c.lower(): c for c in df.columns}\n",
    "        for col in possible_cols:\n",
    "            if col.lower() in col_lower_map:\n",
    "                col_name = col_lower_map[col.lower()]\n",
    "                break\n",
    "    if col_name is None:\n",
    "        print(\"Could not find ingredient column.\")\n",
    "        return\n",
    "\n",
    "    mask = df[col_name].str.contains(r'__[A-Z]+\\d+__', na=False, regex=True)\n",
    "    affected = df[mask][col_name]\n",
    "    all_tokens = affected.str.findall(r'__[A-Z]+\\d+__').explode()\n",
    "    token_counts = all_tokens.value_counts()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ENCODED TOKEN AUDIT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Rows with encoded tokens: {mask.sum():,}\")\n",
    "    print(f\"Unique token types:       {len(token_counts)}\\n\")\n",
    "    print(token_counts.to_string())\n",
    "    print(\"\\n--- EXAMPLE ROWS PER TOKEN ---\")\n",
    "    for token in token_counts.index:\n",
    "        examples = df[df[col_name].str.contains(re.escape(token), na=False)][col_name].head(3).tolist()\n",
    "        print(f\"\\n{token}  (count={token_counts[token]})\")\n",
    "        for ex in examples:\n",
    "            print(f\"  -> {ex}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Entry Point\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        if os.path.exists(LOG_FILE):\n",
    "            os.remove(LOG_FILE)\n",
    "\n",
    "        # audit_encoded_tokens(INPUT_FILE)\n",
    "\n",
    "        all_passed = test_samples()\n",
    "        if not all_passed:\n",
    "            print(\"WARNING: Some sanity checks failed. Review before proceeding.\\n\")\n",
    "\n",
    "        df_result = clean_drug_ingredients(INPUT_FILE, OUTPUT_FILE)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"SAMPLE CLEANED INGREDIENTS (First 15 rows)\")\n",
    "        print(\"=\" * 70)\n",
    "        sample = df_result[['Graph_Node_Ingredient', 'ingredient_count', 'combo_type', 'row_flag']].head(15)\n",
    "        for idx, row in sample.iterrows():\n",
    "            ingredient = row['Graph_Node_Ingredient']\n",
    "            count = row['ingredient_count']\n",
    "            combo = row['combo_type']\n",
    "            flag = row['row_flag'] or ''\n",
    "            display = ingredient if len(ingredient) <= 60 else ingredient[:57] + \"...\"\n",
    "            flag_str = f\" [{flag}]\" if flag else \"\"\n",
    "            print(f\"{idx + 1:2d}. [{combo:6s}] ({count} ing){flag_str} {display}\")\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nFull results saved to : {OUTPUT_FILE}\")\n",
    "        print(f\"Detailed log saved to : {LOG_FILE}\")\n",
    "        print(\"\\nSTATISTICS SUMMARY:\")\n",
    "        print(f\"   Total valid drugs : {len(df_result):,}\")\n",
    "        print(f\"   Single ingredient : {(df_result['combo_type'] == 'single').sum():,}\")\n",
    "        print(f\"   Combinations      : {(df_result['combo_type'] == 'combo').sum():,}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_message(f\"\\nERROR OCCURRED: {e}\")\n",
    "        import traceback\n",
    "        log_message(traceback.format_exc())\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Check log file for details.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
